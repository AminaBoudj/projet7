{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5024493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clause d'importation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878512d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "# Preprocess application_train.csv and application_test.csv\n",
    "def application_train_test(num_rows = None, nan_as_category = False):\n",
    "    # Read data and merge\n",
    "    df = pd.read_csv('application_train.csv', nrows= num_rows)\n",
    "    test_df = pd.read_csv('application_test.csv', nrows= num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index()\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    # Some simple new features (percentages)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance(num_rows = None, nan_as_category = True):\n",
    "    bureau = pd.read_csv('bureau.csv', nrows = num_rows)\n",
    "    bb = pd.read_csv('bureau_balance.csv', nrows = num_rows)\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg\n",
    "\n",
    "# Preprocess previous_applications.csv\n",
    "def previous_applications(num_rows = None, nan_as_category = True):\n",
    "    prev = pd.read_csv('previous_application.csv', nrows = num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg\n",
    "\n",
    "# Preprocess POS_CASH_balance.csv\n",
    "def pos_cash(num_rows = None, nan_as_category = True):\n",
    "    pos = pd.read_csv('POS_CASH_balance.csv', nrows = num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg\n",
    "    \n",
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(num_rows = None, nan_as_category = True):\n",
    "    ins = pd.read_csv('installments_payments.csv', nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg\n",
    "\n",
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(num_rows = None, nan_as_category = True):\n",
    "    cc = pd.read_csv('credit_card_balance.csv', nrows = num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77fc7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 307511, test samples: 48744\n",
      "Bureau df shape: (305811, 116)\n",
      "Process bureau and bureau_balance - done in 16s\n",
      "Previous applications df shape: (338857, 249)\n",
      "Process previous_applications - done in 20s\n",
      "Pos-cash balance df shape: (337252, 18)\n",
      "Process POS-CASH balance - done in 10s\n",
      "Installments payments df shape: (339587, 26)\n",
      "Process installments payments - done in 22s\n",
      "Credit card balance df shape: (103558, 141)\n",
      "Process credit card balance - done in 18s\n",
      "Full model run - done in 90s\n"
     ]
    }
   ],
   "source": [
    "def main(debug = False):\n",
    "    num_rows = 10000 if debug else None\n",
    "    df = application_train_test(num_rows)\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        del cc\n",
    "        gc.collect()\n",
    "    return df\n",
    "if __name__ == \"__main__\":\n",
    "    submission_file_name = \"submission_kernel02.csv\"\n",
    "    with timer(\"Full model run\"):\n",
    "        df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f39f1b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display/plot feature importance\n",
    "def display_importances(importance_df):\n",
    "    # Affichage des fonctionnalités importantes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef36ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cost(y_true, y_pred):\n",
    "    fn_cost = 10\n",
    "    fp_cost = 1\n",
    "    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n",
    "    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n",
    "    score = fn * fn_cost + fp * fp_cost\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a06d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "def fillna_fun(df, target_name='TARGET', threshold=10):\n",
    "    \"\"\"\n",
    "    fill the dataframe,\n",
    "    don't  touch to the TARGET column,\n",
    "    remove col with more than 1/threshold NaN values.\n",
    "    \"\"\"\n",
    "    df_v2 = df.copy()\n",
    "    target = df_v2[target_name]\n",
    "    df_v2 = df_v2.drop(target_name, axis=1)\n",
    "\n",
    "    print('Start with ', (df_v2.isna().sum() > 1 ).sum(), ' columns with NaN and ', df_v2.isna().sum().sum(), ' cells.' , sep=\"\")\n",
    "    for col in df_v2:\n",
    "        if (df_v2[col].isna().sum() > (len(df_v2)/ threshold) ):\n",
    "            df_v2 = df_v2.drop(col, axis=1)\n",
    "    print('Now there is ', (df_v2.isna().sum() > 1 ).sum(), ' columns with NaN and ', df_v2.isna().sum().sum(), ' cells.', sep=\"\")\n",
    "\n",
    "    #Imputing with default parameters\n",
    "    imputer = KNNImputer()\n",
    "\n",
    "    #Reshaping to meet the dimensional requirement\n",
    "    imputer.fit(df_v2)\n",
    "\n",
    "   \n",
    "    df_filled =  pd.DataFrame(imputer.transform(df_v2), columns=df_v2.columns)\n",
    "\n",
    "    return pd.concat([df_filled, target], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6766830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start with 615 columns with NaN and 63543549 cells.\n",
      "Now there is 260 columns with NaN and 4148503 cells.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df.to_csv(\"customer_data\", index= False)\n",
    "\n",
    "# Remplacer les valeurs infinies par NaN dans le DataFrame df\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)       \n",
    "\n",
    "train_df = df[df['TARGET'].notnull()]\n",
    "test_df = df[df['TARGET'].isnull()]\n",
    "    \n",
    "df_fillna= fillna_fun(train_df, target_name='TARGET', threshold=10)       \n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e82735e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "\n",
    "X = df_fillna.drop(['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index'], axis=1, errors=\"ignore\")\n",
    "y = df_fillna['TARGET']\n",
    "    \n",
    "\n",
    "# Separate features and target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab6857e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    282679\n",
      "1.0     24824\n",
      "Name: TARGET, dtype: int64\n",
      "\n",
      "Class 0: 91.93%\n",
      "Class 1: 8.07%\n"
     ]
    }
   ],
   "source": [
    "class_dist = df_fillna['TARGET'].value_counts()\n",
    "\n",
    "class_0_percentage = 100 * class_dist[0] / (class_dist[0] + class_dist[1])\n",
    "class_1_percentage = 100 * class_dist[1] / (class_dist[0] + class_dist[1])\n",
    "\n",
    "print(class_dist)\n",
    "print('\\nClass 0: {:.2f}%'.format(class_0_percentage))\n",
    "print('Class 1: {:.2f}%'.format(class_1_percentage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45a73e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 19772\n",
      "Before OverSampling, counts of label '0': 226230 \n",
      "\n",
      "After OverSampling, the shape of train_X: (339345, 440)\n",
      "After OverSampling, the shape of train_y: (339345,) \n",
      "\n",
      "After OverSampling, counts of label '1': 113115\n",
      "After OverSampling, counts of label '0': 226230\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n",
    "  \n",
    "\n",
    "ros = RandomOverSampler(sampling_strategy=0.5, random_state=100)\n",
    "X_train_res, y_train_res = ros.fit_resample(X_train, y_train.ravel())\n",
    "\n",
    "  \n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf1a4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Fonction objectif pour l'optimisation multi-objectifs\n",
    "def objective(trial):\n",
    "    # Définition des hyperparamètres à optimiser et de leurs espaces de recherche\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [50, 100]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [3, 5]),\n",
    "        'min_samples_split': trial.suggest_categorical('min_samples_split', [2, 5]),\n",
    "        'min_samples_leaf': trial.suggest_categorical('min_samples_leaf', [1, 4]),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt'])\n",
    "    }\n",
    "\n",
    "    # Entraînement du modèle avec les hyperparamètres proposés par Optuna\n",
    "    model = RandomForestClassifier(**params)\n",
    "\n",
    "    # Définition des métriques à évaluer lors de la validation croisée\n",
    "    scoring = {\n",
    "        'auc': make_scorer(roc_auc_score),\n",
    "        'score_metier': make_scorer(custom_cost, greater_is_better=False)\n",
    "    }\n",
    "\n",
    "    # Calcul des scores de validation croisée avec les métriques AUC et score métier\n",
    "    cv_results = cross_validate(model, X_train, y_train, cv=5, scoring=scoring)\n",
    "\n",
    "    # Calcul des métriques moyennes sur les plis de validation croisée\n",
    "    auc = cv_results['test_auc'].mean()\n",
    "    score_metier = -cv_results['test_score_metier'].mean()\n",
    "\n",
    "    # Optimisation multi-objectifs : maximisation de l'AUC et minimisation du score métier\n",
    "    return auc, score_metier\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de validation\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "def random_forest_model(X_train_res, y_train_res, X_test, y_test) :\n",
    "    # Création de l'étude Optuna\n",
    "    study = optuna.create_study(directions=['maximize', 'minimize'])\n",
    "\n",
    "    # Optimisation des hyperparamètres avec Optuna\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    # Obtention des meilleurs essais pour chaque objectif\n",
    "    best_trials = study.best_trials\n",
    "\n",
    "    # Obtention des meilleurs hyperparamètres\n",
    "    best_params = best_trials[0].params\n",
    "\n",
    "    # Terminez l'exécution MLflow précédente si elle est active\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n",
    "\n",
    "    # Démarrez une nouvelle exécution MLflow\n",
    "    mlflow.start_run()\n",
    "\n",
    "    # Enregistrez les paramètres\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Entraînement du modèle avec les meilleurs hyperparamètres pour l'AUC\n",
    "    best_model = RandomForestClassifier(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Enregistrez le modèle\n",
    "    mlflow.sklearn.log_model(best_model, \"random_forest_model\")\n",
    "\n",
    "    # Prédictions sur les données de test\n",
    "    y_pred = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Conversion des probabilités en classes prédites en utilisant un seuil optimal\n",
    "    threshold = 0.5  # Seuil initial\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    score_metier = custom_cost(y_test, y_pred_binary)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    # Enregistrez les métriques\n",
    "    mlflow.log_metrics({\"accuracy\": accuracy, \"score_metier\": score_metier, \"auc\": auc})\n",
    "\n",
    "    # Terminez l'exécution MLflow\n",
    "    mlflow.end_run()\n",
    "    return accuracy, score_metier, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d04b7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-11 22:26:30,683] A new study created in memory with name: no-name-eba74c2a-626d-4ed9-aaf2-327576410d0e\n",
      "[I 2023-07-11 22:28:29,686] Trial 0 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 22:29:44,020] Trial 1 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 22:31:40,537] Trial 2 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 22:32:17,057] Trial 3 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 22:34:13,523] Trial 4 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 22:35:16,934] Trial 5 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 22:37:21,998] Trial 6 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 22:39:26,021] Trial 7 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 22:40:44,484] Trial 8 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 22:41:44,480] Trial 9 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 22:43:41,852] Trial 10 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 22:44:41,078] Trial 11 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 22:45:17,844] Trial 12 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 22:46:16,625] Trial 13 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 22:47:28,080] Trial 14 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 22:48:26,958] Trial 15 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 22:49:25,519] Trial 16 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 22:51:21,439] Trial 17 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 22:51:58,416] Trial 18 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 22:53:54,138] Trial 19 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 22:54:52,968] Trial 20 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 22:55:56,751] Trial 21 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 22:58:01,298] Trial 22 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:00:02,713] Trial 23 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:02:07,623] Trial 24 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:03:20,135] Trial 25 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:03:57,356] Trial 26 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:04:34,620] Trial 27 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:06:37,425] Trial 28 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:07:13,779] Trial 29 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:08:12,799] Trial 30 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:10:11,996] Trial 31 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:11:13,017] Trial 32 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:11:50,613] Trial 33 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:13:49,204] Trial 34 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:14:49,519] Trial 35 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:16:48,018] Trial 36 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:17:51,884] Trial 37 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:19:56,001] Trial 38 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:20:57,439] Trial 39 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:22:09,185] Trial 40 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-11 23:24:13,613] Trial 41 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:26:16,077] Trial 42 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:26:53,419] Trial 43 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:28:08,967] Trial 44 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:30:06,542] Trial 45 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:31:18,787] Trial 46 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:31:57,437] Trial 47 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:33:57,237] Trial 48 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:35:54,349] Trial 49 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:36:52,900] Trial 50 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:38:49,832] Trial 51 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:40:46,185] Trial 52 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:41:44,509] Trial 53 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:42:43,721] Trial 54 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:44:39,211] Trial 55 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:45:16,311] Trial 56 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:46:28,461] Trial 57 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:48:25,536] Trial 58 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:49:24,224] Trial 59 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:50:00,474] Trial 60 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:50:59,174] Trial 61 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:52:10,060] Trial 62 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:53:20,501] Trial 63 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:53:57,315] Trial 64 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:55:53,169] Trial 65 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-11 23:57:48,494] Trial 66 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-11 23:59:43,838] Trial 67 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:00:41,377] Trial 68 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:02:37,873] Trial 69 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-12 00:04:34,156] Trial 70 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-12 00:05:32,603] Trial 71 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-12 00:06:31,637] Trial 72 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-12 00:07:42,326] Trial 73 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:08:41,520] Trial 74 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-12 00:09:58,115] Trial 75 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:11:55,894] Trial 76 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-12 00:13:51,961] Trial 77 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:14:50,066] Trial 78 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:15:48,825] Trial 79 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-12 00:17:45,148] Trial 80 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-12 00:19:40,043] Trial 81 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-12 00:20:50,757] Trial 82 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:22:02,460] Trial 83 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-12 00:23:01,482] Trial 84 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-12 00:24:00,348] Trial 85 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:25:55,597] Trial 86 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-12 00:27:50,986] Trial 87 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:28:49,328] Trial 88 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:29:26,273] Trial 89 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:31:22,724] Trial 90 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-12 00:32:34,389] Trial 91 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:34:30,855] Trial 92 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:35:29,631] Trial 93 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:37:25,735] Trial 94 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto'}. \n",
      "[I 2023-07-12 00:38:37,815] Trial 95 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:39:48,296] Trial 96 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n",
      "[I 2023-07-12 00:40:46,819] Trial 97 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-12 00:42:44,592] Trial 98 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto'}. \n",
      "[I 2023-07-12 00:43:21,394] Trial 99 finished with values: [0.5, 115342.0] and parameters: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. \n"
     ]
    }
   ],
   "source": [
    "accuracy_rf,score_metier_rf,auc_rf = random_forest_model(X_train_res, y_train_res, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "10bc8f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_lightgbm(df, num_folds, stratified = False, debug= False):\n",
    "    # Divide in training/validation and test data\n",
    "   \n",
    "     \n",
    "    if stratified:\n",
    "        cv = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        cv = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "         \n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format( X_train.shape, X_test.shape))\n",
    "\n",
    "\n",
    "    # Paramètres initiaux du modèle LGBM avec la métrique 'auc'\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    # Définition des hyperparamètres à optimiser et de leurs valeurs possibles\n",
    "    param_grid = {\n",
    "        'num_leaves': [20, 30, 40],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'feature_fraction': [0.8, 0.9, 1.0],\n",
    "        'bagging_fraction': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "\n",
    "    # Fonction de coût métier pour utiliser comme métrique de score dans GridSearchCV\n",
    "\n",
    "    # Définition de la métrique de score pour GridSearchCV\n",
    "    scoring = make_scorer(custom_cost, greater_is_better=False)\n",
    "\n",
    "    # Recherche par grille des meilleurs hyperparamètres\n",
    "    grid_search = GridSearchCV(estimator=LGBMClassifier(**params),\n",
    "                           param_grid=param_grid,\n",
    "                           scoring=scoring,\n",
    "                           cv=cv,\n",
    "                           verbose=2)\n",
    "   \n",
    "\n",
    "    # Entraînement du modèle avec les meilleurs hyperparamètres\n",
    "    grid_search.fit( X_train, y_train)\n",
    "\n",
    "    # Obtention du meilleur modèle\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    with open('modele_lightgbm.pkl', 'wb') as f:\n",
    "        pickle.dump( best_model, f)\n",
    "    \n",
    "    \n",
    "    # Obtention de l'importance des fonctionnalités\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    # Création d'un DataFrame pour stocker les fonctionnalités et leur importance\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "    # Tri du DataFrame par importance décroissante\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False).head(10)\n",
    "    display_importances(importance_df)\n",
    "    \n",
    "    # Prédictions sur les données de test\n",
    "    y_pred = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Conversion des probabilités en classes prédites en utilisant un seuil optimal\n",
    "    threshold = 0.5  # Seuil initial\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    print(\"Meilleurs hyperparamètres:\", grid_search.best_params_)\n",
    "\n",
    "    # Calcul des métriques AUC, accuracy et score métier\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    score_metier = custom_cost(y_test, y_pred_binary)\n",
    "    print(\"Score métier:\", score_metier)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    # Affichage des résultats\n",
    "    print(\"AUC:\", auc)\n",
    "      \n",
    "    \n",
    "    return importance_df,accuracy,score_metier,auc\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "005aab39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGBM. Train shape: (246005, 795), test shape: (61502, 795)\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.01, num_leaves=20; total time=  23.8s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.01, num_leaves=20; total time=  22.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.01, num_leaves=20; total time=  20.7s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.01, num_leaves=20; total time=  21.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.01, num_leaves=20; total time=  20.8s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.01, num_leaves=30; total time=  25.9s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.01, num_leaves=30; total time=  25.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.01, num_leaves=30; total time=  24.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.01, num_leaves=30; total time=  24.1s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.01, num_leaves=30; total time=  24.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.01, num_leaves=40; total time=  27.4s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.01, num_leaves=40; total time=  26.8s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.01, num_leaves=40; total time=  27.1s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.01, num_leaves=40; total time=  27.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.01, num_leaves=40; total time=  27.7s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.05, num_leaves=20; total time=  20.7s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.05, num_leaves=20; total time=  20.7s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.05, num_leaves=20; total time=  20.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.05, num_leaves=20; total time=  20.0s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.05, num_leaves=20; total time=  21.4s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.05, num_leaves=30; total time=  24.0s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.05, num_leaves=30; total time=  23.7s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.05, num_leaves=30; total time=  23.7s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.05, num_leaves=30; total time=  23.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.05, num_leaves=30; total time=  23.4s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.05, num_leaves=40; total time=  25.6s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.05, num_leaves=40; total time=  25.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.05, num_leaves=40; total time=  25.1s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.05, num_leaves=40; total time=  25.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.05, num_leaves=40; total time=  25.4s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.1, num_leaves=20; total time=  19.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.1, num_leaves=20; total time=  19.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.1, num_leaves=20; total time=  19.4s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.1, num_leaves=20; total time=  18.9s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.1, num_leaves=20; total time=  19.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.1, num_leaves=30; total time=  21.6s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.1, num_leaves=30; total time=  21.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.1, num_leaves=30; total time=  21.6s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.1, num_leaves=30; total time=  20.9s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.1, num_leaves=30; total time=  21.4s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.1, num_leaves=40; total time=  23.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.1, num_leaves=40; total time=  23.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.1, num_leaves=40; total time=  23.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.1, num_leaves=40; total time=  23.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.8, learning_rate=0.1, num_leaves=40; total time=  23.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.01, num_leaves=20; total time=  22.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.01, num_leaves=20; total time=  22.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.01, num_leaves=20; total time=  22.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.01, num_leaves=20; total time=  22.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.01, num_leaves=20; total time=  22.7s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.01, num_leaves=30; total time=  25.9s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.01, num_leaves=30; total time=  26.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.01, num_leaves=30; total time=  27.6s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.01, num_leaves=30; total time=  26.6s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.01, num_leaves=30; total time=  28.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.01, num_leaves=40; total time=  31.4s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.01, num_leaves=40; total time=  29.9s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.01, num_leaves=40; total time=  30.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.01, num_leaves=40; total time=  31.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.01, num_leaves=40; total time=  29.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.05, num_leaves=20; total time=  21.7s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.05, num_leaves=20; total time=  21.6s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.05, num_leaves=20; total time=  22.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.05, num_leaves=20; total time=  22.0s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.05, num_leaves=20; total time=  22.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.05, num_leaves=30; total time=  25.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.05, num_leaves=30; total time=  28.0s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.05, num_leaves=30; total time=  26.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.05, num_leaves=30; total time=  25.1s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.05, num_leaves=30; total time=  25.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.05, num_leaves=40; total time=  27.7s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.05, num_leaves=40; total time=  27.9s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.05, num_leaves=40; total time=  29.1s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.05, num_leaves=40; total time=  27.8s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.05, num_leaves=40; total time=  29.1s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.1, num_leaves=20; total time=  22.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.1, num_leaves=20; total time=  21.1s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.1, num_leaves=20; total time=  21.9s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.1, num_leaves=20; total time=  21.7s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.1, num_leaves=20; total time=  22.4s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.1, num_leaves=30; total time=  23.6s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.1, num_leaves=30; total time=  23.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.1, num_leaves=30; total time=  22.9s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.1, num_leaves=30; total time=  22.7s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.1, num_leaves=30; total time=  22.8s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.1, num_leaves=40; total time=  24.8s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.1, num_leaves=40; total time=  25.1s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.1, num_leaves=40; total time=  25.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.1, num_leaves=40; total time=  24.7s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=0.9, learning_rate=0.1, num_leaves=40; total time=  25.1s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.01, num_leaves=20; total time=  23.9s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.01, num_leaves=20; total time=  24.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.01, num_leaves=20; total time=  24.0s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.01, num_leaves=20; total time=  24.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.01, num_leaves=20; total time=  24.1s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.01, num_leaves=30; total time=  28.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.01, num_leaves=30; total time=  28.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.01, num_leaves=30; total time=  28.1s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.01, num_leaves=30; total time=  27.8s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.01, num_leaves=30; total time=  28.1s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.01, num_leaves=40; total time=  31.6s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.01, num_leaves=40; total time=  31.8s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.01, num_leaves=40; total time=  31.9s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.01, num_leaves=40; total time=  31.0s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.01, num_leaves=40; total time=  31.4s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.05, num_leaves=20; total time=  23.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.05, num_leaves=20; total time=  23.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.05, num_leaves=20; total time=  23.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.05, num_leaves=20; total time=  23.0s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.05, num_leaves=20; total time=  22.9s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.05, num_leaves=30; total time=  26.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.05, num_leaves=30; total time=  26.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.05, num_leaves=30; total time=  26.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.05, num_leaves=30; total time=  26.1s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.05, num_leaves=30; total time=  26.8s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.05, num_leaves=40; total time=  29.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.05, num_leaves=40; total time=  29.6s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.05, num_leaves=40; total time=  29.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.05, num_leaves=40; total time=  28.8s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.05, num_leaves=40; total time=  29.2s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.1, num_leaves=20; total time=  21.6s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.1, num_leaves=20; total time=  21.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.1, num_leaves=20; total time=  21.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.1, num_leaves=20; total time=  21.8s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.1, num_leaves=20; total time=  22.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.1, num_leaves=30; total time=  24.4s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.1, num_leaves=30; total time=  24.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.1, num_leaves=30; total time=  24.3s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.1, num_leaves=30; total time=  24.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.1, num_leaves=30; total time=  24.5s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.1, num_leaves=40; total time=  26.6s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.1, num_leaves=40; total time=  26.7s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.1, num_leaves=40; total time=  27.1s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.1, num_leaves=40; total time=  27.4s\n",
      "[CV] END bagging_fraction=0.7, feature_fraction=1.0, learning_rate=0.1, num_leaves=40; total time=  27.1s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.01, num_leaves=20; total time=  22.5s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.01, num_leaves=20; total time=  22.6s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.01, num_leaves=20; total time=  22.6s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.01, num_leaves=20; total time=  22.5s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.01, num_leaves=20; total time=  22.7s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.01, num_leaves=30; total time=  26.0s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.01, num_leaves=30; total time=  26.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.01, num_leaves=30; total time=  26.0s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.01, num_leaves=30; total time=  25.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.01, num_leaves=30; total time=  26.4s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.01, num_leaves=40; total time=  29.1s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.01, num_leaves=40; total time=  29.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.01, num_leaves=40; total time=  29.4s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.01, num_leaves=40; total time=  29.4s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.01, num_leaves=40; total time=  29.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.05, num_leaves=20; total time=  22.0s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.05, num_leaves=20; total time=  21.7s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.05, num_leaves=20; total time=  21.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.05, num_leaves=20; total time=  21.6s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.05, num_leaves=20; total time=  22.0s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.05, num_leaves=30; total time=  25.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.05, num_leaves=30; total time=  24.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.05, num_leaves=30; total time=  24.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.05, num_leaves=30; total time=  24.5s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.05, num_leaves=30; total time=  24.9s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.05, num_leaves=40; total time=  27.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.05, num_leaves=40; total time=  27.0s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.05, num_leaves=40; total time=  27.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.05, num_leaves=40; total time=  27.1s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.05, num_leaves=40; total time=  27.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.1, num_leaves=20; total time=  20.6s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.1, num_leaves=20; total time=  20.4s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.1, num_leaves=20; total time=  20.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.1, num_leaves=20; total time=  20.5s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.1, num_leaves=20; total time=  20.4s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.1, num_leaves=30; total time=  22.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.1, num_leaves=30; total time=  23.0s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.1, num_leaves=30; total time=  22.5s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.1, num_leaves=30; total time=  22.9s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.1, num_leaves=30; total time=  22.7s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.1, num_leaves=40; total time=  25.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.1, num_leaves=40; total time=  24.7s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.1, num_leaves=40; total time=  24.7s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.1, num_leaves=40; total time=  24.7s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.8, learning_rate=0.1, num_leaves=40; total time=  25.0s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.01, num_leaves=20; total time=  24.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.01, num_leaves=20; total time=  24.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.01, num_leaves=20; total time=  24.1s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.01, num_leaves=20; total time=  24.5s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.01, num_leaves=20; total time=  24.7s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.01, num_leaves=30; total time=  28.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.01, num_leaves=30; total time=  29.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.01, num_leaves=30; total time=  29.1s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.01, num_leaves=30; total time=  27.9s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.01, num_leaves=30; total time=  28.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.01, num_leaves=40; total time=  31.9s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.01, num_leaves=40; total time=  31.6s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.01, num_leaves=40; total time=  31.7s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.01, num_leaves=40; total time=  31.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.01, num_leaves=40; total time=  31.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.05, num_leaves=20; total time=  23.6s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.05, num_leaves=20; total time=  24.0s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.05, num_leaves=20; total time=  23.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.05, num_leaves=20; total time=  25.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.05, num_leaves=20; total time=  27.1s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.05, num_leaves=30; total time=  27.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.05, num_leaves=30; total time=  26.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.05, num_leaves=30; total time=  26.5s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.05, num_leaves=30; total time=  26.7s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.05, num_leaves=30; total time=  26.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.05, num_leaves=40; total time=  29.6s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.05, num_leaves=40; total time=  29.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.05, num_leaves=40; total time=  29.6s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.05, num_leaves=40; total time=  29.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.05, num_leaves=40; total time=  30.0s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.1, num_leaves=20; total time=  22.9s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.1, num_leaves=20; total time=  23.1s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.1, num_leaves=20; total time=  22.4s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.1, num_leaves=20; total time=  22.4s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.1, num_leaves=20; total time=  22.4s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.1, num_leaves=30; total time=  25.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.1, num_leaves=30; total time=  25.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.1, num_leaves=30; total time=  24.6s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.1, num_leaves=30; total time=  24.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.1, num_leaves=30; total time=  24.6s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.1, num_leaves=40; total time=  26.6s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.1, num_leaves=40; total time=  26.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.1, num_leaves=40; total time=  26.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.1, num_leaves=40; total time=  26.5s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=0.9, learning_rate=0.1, num_leaves=40; total time=  26.9s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.01, num_leaves=20; total time=  26.1s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.01, num_leaves=20; total time=  26.7s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.01, num_leaves=20; total time=  26.4s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.01, num_leaves=20; total time=  26.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.01, num_leaves=20; total time=  26.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.01, num_leaves=30; total time=  29.9s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.01, num_leaves=30; total time=  30.4s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.01, num_leaves=30; total time=  32.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.01, num_leaves=30; total time=  30.1s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.01, num_leaves=30; total time=  30.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.01, num_leaves=40; total time=  33.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.01, num_leaves=40; total time=  34.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.01, num_leaves=40; total time=  34.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.01, num_leaves=40; total time=  33.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.01, num_leaves=40; total time=  34.1s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.05, num_leaves=20; total time=  24.9s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.05, num_leaves=20; total time=  24.9s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.05, num_leaves=20; total time=  25.1s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.05, num_leaves=20; total time=  24.9s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.05, num_leaves=20; total time=  25.4s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.05, num_leaves=30; total time=  28.9s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.05, num_leaves=30; total time=  28.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.05, num_leaves=30; total time=  29.0s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.05, num_leaves=30; total time=  28.6s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.05, num_leaves=30; total time=  28.7s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.05, num_leaves=40; total time=  31.5s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.05, num_leaves=40; total time=  31.4s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.05, num_leaves=40; total time=  31.9s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.05, num_leaves=40; total time=  31.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.05, num_leaves=40; total time=  31.5s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.1, num_leaves=20; total time=  23.1s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.1, num_leaves=20; total time=  23.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.1, num_leaves=20; total time=  23.0s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.1, num_leaves=20; total time=  23.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.1, num_leaves=20; total time=  23.4s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.1, num_leaves=30; total time=  26.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.1, num_leaves=30; total time=  25.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.1, num_leaves=30; total time=  26.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.1, num_leaves=30; total time=  26.2s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.1, num_leaves=30; total time=  26.0s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.1, num_leaves=40; total time=  28.6s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.1, num_leaves=40; total time=  28.7s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.1, num_leaves=40; total time=  28.8s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.1, num_leaves=40; total time=  28.3s\n",
      "[CV] END bagging_fraction=0.8, feature_fraction=1.0, learning_rate=0.1, num_leaves=40; total time=  28.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.01, num_leaves=20; total time=  24.1s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.01, num_leaves=20; total time=  24.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.01, num_leaves=20; total time=  23.8s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.01, num_leaves=20; total time=  24.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.01, num_leaves=20; total time=  24.3s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.01, num_leaves=30; total time=  28.1s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.01, num_leaves=30; total time=  27.9s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.01, num_leaves=30; total time=  28.1s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.01, num_leaves=30; total time=  29.0s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.01, num_leaves=30; total time=  28.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.01, num_leaves=40; total time=  31.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.01, num_leaves=40; total time=  31.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.01, num_leaves=40; total time=  31.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.01, num_leaves=40; total time=  30.9s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.01, num_leaves=40; total time=  32.1s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.05, num_leaves=20; total time=  24.1s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.05, num_leaves=20; total time=  23.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.05, num_leaves=20; total time=  23.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.05, num_leaves=20; total time=  23.3s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.05, num_leaves=20; total time=  23.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.05, num_leaves=30; total time=  26.7s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.05, num_leaves=30; total time=  26.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.05, num_leaves=30; total time=  26.8s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.05, num_leaves=30; total time=  26.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.05, num_leaves=30; total time=  26.7s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.05, num_leaves=40; total time=  29.4s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.05, num_leaves=40; total time=  29.4s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.05, num_leaves=40; total time=  29.4s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.05, num_leaves=40; total time=  29.1s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.05, num_leaves=40; total time=  29.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.1, num_leaves=20; total time=  22.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.1, num_leaves=20; total time=  22.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.1, num_leaves=20; total time=  22.4s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.1, num_leaves=20; total time=  21.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.1, num_leaves=20; total time=  22.0s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.1, num_leaves=30; total time=  24.4s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.1, num_leaves=30; total time=  26.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.1, num_leaves=30; total time=  25.3s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.1, num_leaves=30; total time=  24.5s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.1, num_leaves=30; total time=  24.5s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.1, num_leaves=40; total time=  27.1s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.1, num_leaves=40; total time=  27.5s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.1, num_leaves=40; total time=  27.3s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.1, num_leaves=40; total time=  27.0s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.8, learning_rate=0.1, num_leaves=40; total time=  27.5s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.01, num_leaves=20; total time=  26.4s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.01, num_leaves=20; total time=  26.4s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.01, num_leaves=20; total time=  26.9s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.01, num_leaves=20; total time=  27.3s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.01, num_leaves=20; total time=  27.0s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.01, num_leaves=30; total time=  30.5s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.01, num_leaves=30; total time=  30.9s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.01, num_leaves=30; total time=  30.9s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.01, num_leaves=30; total time=  31.1s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.01, num_leaves=30; total time=  31.0s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.01, num_leaves=40; total time=  36.5s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.01, num_leaves=40; total time=  35.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.01, num_leaves=40; total time=  34.5s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.01, num_leaves=40; total time=  35.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.01, num_leaves=40; total time=  34.5s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.05, num_leaves=20; total time=  25.9s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.05, num_leaves=20; total time=  25.7s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.05, num_leaves=20; total time=  26.1s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.05, num_leaves=20; total time=  27.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.05, num_leaves=20; total time=  27.1s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.05, num_leaves=30; total time=  30.0s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.05, num_leaves=30; total time=  30.8s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.05, num_leaves=30; total time=  30.4s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.05, num_leaves=30; total time=  31.9s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.05, num_leaves=30; total time=  31.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.05, num_leaves=40; total time=  32.5s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.05, num_leaves=40; total time=  34.7s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.05, num_leaves=40; total time=  33.8s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.05, num_leaves=40; total time=  35.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.05, num_leaves=40; total time=  33.7s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.1, num_leaves=20; total time=  23.8s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.1, num_leaves=20; total time=  24.8s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.1, num_leaves=20; total time=  24.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.1, num_leaves=20; total time=  24.3s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.1, num_leaves=20; total time=  25.7s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.1, num_leaves=30; total time=  27.3s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.1, num_leaves=30; total time=  27.9s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.1, num_leaves=30; total time=  27.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.1, num_leaves=30; total time=  29.8s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.1, num_leaves=30; total time=  28.4s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.1, num_leaves=40; total time=  30.9s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.1, num_leaves=40; total time=  30.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.1, num_leaves=40; total time=  29.8s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.1, num_leaves=40; total time=  29.3s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=0.9, learning_rate=0.1, num_leaves=40; total time=  29.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.01, num_leaves=20; total time=  28.8s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.01, num_leaves=20; total time=  29.4s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.01, num_leaves=20; total time=  30.4s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.01, num_leaves=20; total time=  29.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.01, num_leaves=20; total time=  27.8s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.01, num_leaves=30; total time=  32.7s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.01, num_leaves=30; total time=  33.0s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.01, num_leaves=30; total time=  33.0s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.01, num_leaves=30; total time=  33.1s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.01, num_leaves=30; total time=  33.1s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.01, num_leaves=40; total time=  37.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.01, num_leaves=40; total time=  37.7s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.01, num_leaves=40; total time=  37.7s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.01, num_leaves=40; total time=  37.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.01, num_leaves=40; total time=  37.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.05, num_leaves=20; total time=  27.3s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.05, num_leaves=20; total time=  27.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.05, num_leaves=20; total time=  27.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.05, num_leaves=20; total time=  27.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.05, num_leaves=20; total time=  27.3s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.05, num_leaves=30; total time=  31.5s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.05, num_leaves=30; total time=  31.5s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.05, num_leaves=30; total time=  31.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.05, num_leaves=30; total time=  31.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.05, num_leaves=30; total time=  31.4s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.05, num_leaves=40; total time=  34.8s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.05, num_leaves=40; total time=  34.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.05, num_leaves=40; total time=  35.0s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.05, num_leaves=40; total time=  34.8s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.05, num_leaves=40; total time=  35.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.1, num_leaves=20; total time=  25.3s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.1, num_leaves=20; total time=  25.1s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.1, num_leaves=20; total time=  26.1s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.1, num_leaves=20; total time=  25.3s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.1, num_leaves=20; total time=  25.4s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.1, num_leaves=30; total time=  28.7s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.1, num_leaves=30; total time=  28.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.1, num_leaves=30; total time=  28.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.1, num_leaves=30; total time=  29.5s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.1, num_leaves=30; total time=  29.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.1, num_leaves=40; total time=  32.6s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.1, num_leaves=40; total time=  31.5s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.1, num_leaves=40; total time=  31.5s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.1, num_leaves=40; total time=  31.2s\n",
      "[CV] END bagging_fraction=0.9, feature_fraction=1.0, learning_rate=0.1, num_leaves=40; total time=  31.6s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABB4AAAIhCAYAAADpWnlxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAACCz0lEQVR4nOzdeXwNZ///8feRE4k4SQjVJKSoIIilLaqqltq3onZCYi1FqbXR3qgttFVRaqlGQquIUkVbRe2ttW3UVvSuVEu0vS0JQWQ5vz/8cr5OT5aTyJHi9Xw85vGVa66Z+cxM7vt7zzvXXGMwm81mAQAAAAAAOECB/C4AAAAAAAA8uAgeAAAAAACAwxA8AAAAAAAAhyF4AAAAAAAADkPwAAAAAAAAHIbgAQAAAAAAOAzBAwAAAAAAcBiCBwAAAAAA4DAEDwAAAAAAwGEIHgAAwEMnKipKBoMhw2X06NEOOebx48c1adIkxcbGOmT/dyM2NlYGg0HvvPNOfpeSa999950mTZqkK1eu5HcpAIB/MOZ3AQAAAPklMjJSAQEBVm2+vr4OOdbx48f15ptvqmHDhipTpoxDjvEw++677/Tmm28qJCRERYoUye9yAAB3IHgAAAAPrcDAQNWsWTO/y7grycnJMhgMMhofzv9Zd+PGDbm6uuZ3GQCALPCqBQAAQCZWrVqlZ555RoULF5bJZFLz5s31448/WvU5dOiQunXrpjJlyqhQoUIqU6aMunfvrt9++83SJyoqSp07d5YkNWrUyPJaR1RUlCSpTJkyCgkJsTl+w4YN1bBhQ8vPO3bskMFg0EcffaRRo0apZMmScnFx0S+//CJJ2rp1qxo3biwPDw+5ubnp2Wef1TfffJOrc09/HWXbtm0aMGCAihUrJg8PD/Xu3VuJiYm6cOGCunTpoiJFisjHx0ejR49WcnKyZfv01zfeeustTZs2TY899phcXV1Vs2bNDGvas2ePGjduLHd3d7m5ualu3br64osvMqxp8+bN6tu3rx555BG5ubkpNDRUY8aMkSSVLVvWcn137Ngh6fZ9bNasmXx8fFSoUCFVqlRJr732mhITE632HxISIpPJpF9++UWtWrWSyWSSn5+fRo0apaSkJKu+SUlJmjx5sipVqiRXV1cVK1ZMjRo10nfffWfpYzabNX/+fNWoUUOFChVS0aJF1alTJ/3666+5uicAcL8ieAAAAA+t1NRUpaSkWC3ppk+fru7du6ty5cqKjo7WRx99pKtXr+q5557T8ePHLf1iY2NVsWJFhYeH6+uvv9bMmTMVFxenWrVq6X//+58kqXXr1po+fbok6f3339fevXu1d+9etW7dOld1h4aG6uzZs1q4cKE2bNigEiVK6OOPP1azZs3k4eGhpUuXKjo6Wl5eXmrevHmuwwdJ6t+/vzw9PbVy5Uq98cYb+uSTTzRgwAC1bt1a1atX16effqrg4GDNmjVLc+fOtdl+3rx52rRpk8LDw/Xxxx+rQIECatmypfbu3Wvps3PnTj3//POKj49XRESEVqxYIXd3d7Vt21arVq2y2Wffvn3l7Oysjz76SJ9++qkGDx6sYcOGSZLWrl1rub5PPvmkJOn06dNq1aqVIiIitGnTJo0YMULR0dFq27atzb6Tk5P1wgsvqHHjxvr888/Vt29fzZ49WzNnzrT0SUlJUcuWLTVlyhS1adNGn332maKiolS3bl2dPXvW0u+ll17SiBEj1KRJE61bt07z58/XsWPHVLduXf3555+5vicAcN8xAwAAPGQiIyPNkjJckpOTzWfPnjUbjUbzsGHDrLa7evWq2dvb29ylS5dM952SkmK+du2auXDhwuY5c+ZY2levXm2WZN6+fbvNNqVLlzYHBwfbtDdo0MDcoEEDy8/bt283SzLXr1/fql9iYqLZy8vL3LZtW6v21NRUc/Xq1c21a9fO4mqYzWfOnDFLMr/99tuWtvRr9M9r0L59e7Mk87vvvmvVXqNGDfOTTz5ps09fX1/zjRs3LO0JCQlmLy8vc5MmTSxtderUMZcoUcJ89epVS1tKSoo5MDDQXKpUKXNaWppVTb1797Y5h7ffftssyXzmzJkszzUtLc2cnJxs3rlzp1mS+fDhw5Z1wcHBZknm6Ohoq21atWplrlixouXnZcuWmSWZFy9enOlx9u7da5ZknjVrllX777//bi5UqJB57NixWdYJAA8SRjwAAICH1rJly3Tw4EGrxWg06uuvv1ZKSop69+5tNRrC1dVVDRo0sAzhl6Rr165p3Lhx8vf3l9FolNFolMlkUmJiok6cOOGQujt27Gj183fffadLly4pODjYqt60tDS1aNFCBw8etHmtwF5t2rSx+rlSpUqSZDNao1KlSlavl6R78cUXreZgSB/JsGvXLqWmpioxMVH79+9Xp06dZDKZLP2cnJzUq1cv/fHHHzp58mSW55+dX3/9VT169JC3t7ecnJzk7OysBg0aSJLNPTIYDDYjIapVq2Z1bl999ZVcXV3Vt2/fTI+5ceNGGQwGBQUFWd0Tb29vVa9e3ep3CAAedA/nLEQAAAC6/bCc0eSS6cPga9WqleF2BQr8399uevTooW+++Ub/+c9/VKtWLXl4eMhgMKhVq1a6ceOGQ+r28fHJsN5OnTplus2lS5dUuHDhHB/Ly8vL6ueCBQtm2n7z5k2b7b29vTNsu3Xrlq5du6arV6/KbDbbnJP0f18YuXjxolV7Rn0zc+3aNT333HNydXXV1KlTVaFCBbm5uen333/Xiy++aHOP3NzcbCardHFxsTq3v//+W76+vla/B//0559/ymw269FHH81w/eOPP273OQDA/Y7gAQAA4B+KFy8uSfr0009VunTpTPvFx8dr48aNmjhxol577TVLe1JSki5dumT38VxdXW0mL5Sk//3vf5Za7mQwGDKsd+7cuapTp06Gx8jsAdjRLly4kGFbwYIFZTKZZDQaVaBAAcXFxdn0O3/+vCTZXIN/nn9Wtm3bpvPnz2vHjh2WUQ6SdOXKFbv38U+PPPKI9uzZo7S0tEzDh+LFi8tgMGj37t1ycXGxWZ9RGwA8qAgeAAAA/qF58+YyGo3673//m+WwfoPBILPZbPMQ+eGHHyo1NdWqLb1PRqMgypQpo59++smq7dSpUzp58mSGwcM/PfvssypSpIiOHz+uoUOHZtv/Xlq7dq3efvttyyiCq1evasOGDXruuefk5OSkwoUL6+mnn9batWv1zjvvqFChQpKktLQ0ffzxxypVqpQqVKiQ7XEyu77pIcU/79GiRYtyfU4tW7bUihUrFBUVlenrFm3atNGMGTN07tw5denSJdfHAoAHAcEDAADAP5QpU0aTJ0/W66+/rl9//VUtWrRQ0aJF9eeff+rAgQMqXLiw3nzzTXl4eKh+/fp6++23Vbx4cZUpU0Y7d+5URESEihQpYrXPwMBASdIHH3wgd3d3ubq6qmzZsipWrJh69eqloKAgvfzyy+rYsaN+++03vfXWW3rkkUfsqtdkMmnu3LkKDg7WpUuX1KlTJ5UoUUJ///23Dh8+rL///lsLFizI68tkFycnJzVt2lQjR45UWlqaZs6cqYSEBL355puWPmFhYWratKkaNWqk0aNHq2DBgpo/f76OHj2qFStW2DXCoWrVqpKkOXPmKDg4WM7OzqpYsaLq1q2rokWLatCgQZo4caKcnZ21fPlyHT58ONfn1L17d0VGRmrQoEE6efKkGjVqpLS0NO3fv1+VKlVSt27d9Oyzz2rgwIHq06ePDh06pPr166tw4cKKi4vTnj17VLVqVQ0ePDjXNQDA/YTJJQEAADIQGhqqTz/9VKdOnVJwcLCaN2+usWPH6rffflP9+vUt/T755BM1atRIY8eO1YsvvqhDhw5py5Yt8vT0tNpf2bJlFR4ersOHD6thw4aqVauWNmzYIOn2PBFvvfWWvv76a7Vp00YLFizQggUL7PpLf7qgoCBt375d165d00svvaQmTZpo+PDh+uGHH9S4ceO8uSi5MHToUDVt2lSvvPKKevTooZSUFH3xxRd69tlnLX0aNGigbdu2qXDhwgoJCVG3bt0UHx+v9evXq2vXrnYdp2HDhgoNDdWGDRtUr1491apVS99//72KFSumL774Qm5ubgoKClLfvn1lMpky/EynvYxGo7788kuFhobqs88+U7t27dS7d2/t2bPH6tWcRYsWad68edq1a5e6deum1q1ba8KECUpMTFTt2rVzfXwAuN8YzGazOb+LAAAAwIMlNjZWZcuW1dtvv63Ro0fndzkAgHzEiAcAAAAAAOAwBA8AAAAAAMBheNUCAAAAAAA4DCMeAAAAAACAwxA8AAAAAAAAhyF4AAAAAAAADmPM7wIA3F/S0tJ0/vx5ubu7y2Aw5Hc5AAAAAPKJ2WzW1atX5evrqwIFMh/XQPAAIEfOnz8vPz+//C4DAAAAwL/E77//rlKlSmW6nuABQI64u7tLuv1fLh4eHvlcDQAAAID8kpCQID8/P8szQmYIHgDkSPrrFR4eHgQPAAAAALJ9BZvJJQEAAAAAgMMQPAAAAAAAAIcheAAAAAAAAA5D8AAAAAAAAByG4AEAAAAAADgMwQMAAAAAAHAYggcAAAAAAOAwBA8AAAAAAMBhCB4AAAAAAIDDEDwAAAAAAACHIXgAAAAAAAAOQ/AAAAAAAAAchuABAAAAAAA4DMEDAAAAAABwGIIHAAAAAADgMAQPAAAAAADAYQgeAAAAAACAwxA8AAAAAAAAhzHmdwEA7k+BE79WARe3/C4DgIPEzmid3yUAAIAHBCMeAAAAAACAwxA8AAAAAAAAhyF4AAAAAAAADkPwAAAAAAAAHIbgAQAAAAAAOAzBAwAAAAAAcBiCBwAAAAAA4DAEDwAAAAAAwGEIHgAAAAAAgMMQPAAAAAAAAIcheAAAAAAAAA5D8AAAAAAAAByG4AEKCQlR+/btLf82GAyaMWOGVZ9169bJYDBYtS1atEjVq1dX4cKFVaRIET3xxBOaOXOmJKlMmTIyGAyZLg0bNrTsZ+DAgXJyctLKlSttaps0aZJq1KiRq/Nq2LCh5XguLi4qWbKk2rZtq7Vr19r0vbM2d3d31axZ06rfpEmTLOuNRqOKFy+u+vXrKzw8XElJSTmu6Z/XV5JatWolg8GgSZMmZXgOdy6DBg2y2T6765jRdjExMTIYDIqNjbX7HAAAAAAgJwgeYMPV1VUzZ87U5cuXM+0TERGhkSNH6pVXXtHhw4f17bffauzYsbp27Zok6eDBg4qLi1NcXJzWrFkjSTp58qSlLf2h/vr161q1apXGjBmjiIiIPD+XAQMGKC4uTr/88ovWrFmjypUrq1u3bho4cKBN38jISMXFxengwYOqXr26OnfurL1791rWV6lSRXFxcTp79qy2b9+uzp07KywsTHXr1tXVq1ftrsnPz0+RkZFWbefPn9e2bdvk4+OT6Tncubz11ltWfey5jq6uroqIiNCpU6fsrhUAAAAA7pYxvwvAv0+TJk30yy+/KCwszOYBN92GDRvUpUsX9evXz9JWpUoVy78feeQRy7+9vLwkSSVKlFCRIkWs9rN69WpVrlxZoaGh8vHxUWxsrMqUKZNn5+Lm5iZvb29Jtx/469Spo4CAAPXt21ddunRRkyZNLH2LFCkib29veXt7a+HChVq5cqXWr1+vZ555RpJkNBot+/L19VXVqlXVtGlTVa9eXTNnztTUqVPtqqlNmzaKjo7Wt99+q2effVaSFBUVpWbNmuns2bNZnkNm7LmOFStWVIkSJfTGG28oOjrarloBAAAA4G4x4gE2nJycNH36dM2dO1d//PFHhn28vb21b98+/fbbb3d1rIiICAUFBcnT01OtWrWyGQngCMHBwSpatGiGr1ykc3Z2ltFoVHJycpb7CggIUMuWLbPc1z8VLFhQPXv2tDrXqKgo9e3b1+59/JO913HGjBlas2aNDh48aPe+k5KSlJCQYLUAAAAAgL0IHpChDh06qEaNGpo4cWKG6ydOnKgiRYqoTJkyqlixokJCQhQdHa20tDS7j3H69Gnt27dPXbt2lSQFBQUpMjIyR/vIjQIFCqhChQqZzmuQlJSkqVOnKiEhQY0bN852fwEBATmeI6Ffv36Kjo5WYmKidu3apfj4eLVu3TrDvvPnz5fJZLJali5dalmfk+v45JNPqkuXLnrttdfsrjUsLEyenp6Wxc/PL0fnCgAAAODhRvCATM2cOVNLly7V8ePHbdb5+Pho7969OnLkiF555RUlJycrODhYLVq0sDs4iIiIUPPmzVW8eHFJtydXTExM1NatW/P0PDJiNpttJsvs3r27TCaT3Nzc9O677+qdd95Ry5Ytc7Wv7FSrVk3ly5fXp59+qiVLlqhXr15ydnbOsG/Pnj0VExNjtXTo0MGyPqfXcerUqdq9e7c2b95sV62hoaGKj4+3LL///nuOzhUAAADAw405HpCp+vXrq3nz5ho/frxCQkIy7BMYGKjAwEANGTJEe/bs0XPPPaedO3eqUaNGWe47NTVVy5Yt04ULF2Q0Gq3aIyIi1KxZs7w8FZtjnz59WrVq1bJqnz17tpo0aSIPDw+VKFHC7v2dOHFCZcuWzXEdffv21fvvv6/jx4/rwIEDmfbz9PSUv79/hutycx3LlSunAQMG6LXXXrNrQk8XFxe5uLjYcUYAAAAAYIvgAVmaMWOGatSooQoVKmTbt3LlypKkxMTEbPt++eWXunr1qn788Uc5OTlZ2n/++Wf17NlTFy9eVLFixXJfeBaWLl2qy5cvq2PHjlbt3t7emT7gZ+bnn3/Wpk2bFBoamuM6evToodGjR6t69eqWa5dTub2OEyZMULly5TL89CYAAAAA5CWCB2SpatWq6tmzp+bOnWvVPnjwYPn6+ur5559XqVKlFBcXp6lTp+qRRx6xfAUiKxEREWrdurWqV69u1V6lShWNGDFCH3/8sYYPHy5JunHjhmJiYqz6mUwmu0KC69ev68KFC0pJSdG5c+e0du1azZ49W4MHD852VMY/paSk6MKFC0pLS9PFixe1Y8cOTZ06VTVq1NCYMWNytC9JKlq0qOLi4jJ9xeKf53AnFxcXFS1aNEfX8U6PPvqoRo4cqbfffjvHdQMAAABATjDHA7I1ZcoUmc1mq7YmTZpo37596ty5sypUqKCOHTvK1dVV33zzTbYjFf7880998cUXNiMOJMlgMOjFF1+0egXg1KlTeuKJJ6yW/v3721X74sWL5ePjo3LlyqlDhw46fvy4Vq1apfnz59u1/Z2OHTsmHx8fPfbYY2rYsKGio6MVGhqq3bt3y2Qy5Xh/0u1PeBYuXNiuc7hz6d69e46v4z+NGTMm13UDAAAAgL0M5n8+UQJAFhISEm5/3WJEtAq4uOV3OQAcJHZGxl/aAQAASJf+bBAfHy8PD49M+zHiAQAAAAAAOAzBA+5L6a83ZLZQEwAAAAD8OzC5JO5LNWvWtJlwMr/9G2sCAAAAgPxG8ID7UqFChXL86UtH+zfWBAAAAAD5jVctAAAAAACAwxA8AAAAAAAAhyF4AAAAAAAADkPwAAAAAAAAHIbgAQAAAAAAOAzBAwAAAAAAcBg+pwkgV46+2VweHh75XQYAAACAfzlGPAAAAAAAAIcheAAAAAAAAA5D8AAAAAAAAByG4AEAAAAAADgMwQMAAAAAAHAYggcAAAAAAOAwBA8AAAAAAMBhCB4AAAAAAIDDGPO7AAD3p8CJX6uAi1t+lwHgARQ7o3V+lwAAAPIQIx4AAAAAAIDDEDwAAAAAAACHIXgAAAAAAAAOQ/AAAAAAAAAchuABAAAAAAA4DMEDAAAAAABwGIIHAAAAAADgMAQPAAAAAADAYQgeAAAAAACAwxA8AAAAAAAAhyF4AAAAAAAADkPwAAAAAAAAHOZfETx89913cnJyUosWLazaY2NjZTAYLEvRokVVv3597dy509InJCTEst7Z2VmPP/64Ro8ercTExAz34enpqTp16mjDhg02ddy4cUMTJ05UxYoV5eLiouLFi6tTp046duyYpU/VqlXVv3//DM9jxYoVcnZ21p9//qkdO3ZYHffO5cKFC5KkSZMmWdqMRqOKFy+u+vXrKzw8XElJSTm6hr/88ov69OmjUqVKycXFRWXLllX37t116NAhSx+DwSBXV1f99ttvVtu2b99eISEhlj5ZLen9snJnf3d3d9WsWVNr16616nPjxg0VLVpUXl5eunHjhiTp1KlTcnNz0yeffGLVNy0tTXXr1lWHDh0k/d89HzRokM2xX375ZZs67/wduXO58/etTJkyMhgM2rdvn9X+RowYoYYNG1r1yWxJ75eV9H2sXLnSZl2VKlVkMBgUFRVl0/+fy4wZM2y2b9asmZycnGzO4c5r8M/t1q1bJ4PBkG3dAAAAAJBb/4rgYcmSJRo2bJj27Nmjs2fP2qzfunWr4uLitHPnTnl4eKhVq1Y6c+aMZX2LFi0UFxenX3/9VVOnTtX8+fM1evToDPexf/9+1a5dWx07dtTRo0ct65OSktSkSRMtWbJEU6ZM0alTp/Tll18qNTVVTz/9tOVhrl+/foqOjtb169czPI82bdro0UcftbSdPHlScXFxVkuJEiUs66tUqaK4uDidPXtW27dvV+fOnRUWFqa6devq6tWrdl2/Q4cO6amnntKpU6e0aNEiHT9+XJ999pkCAgI0atQoq74Gg0ETJkzIdF931hkeHi4PDw+rtjlz5thVU2RkpOLi4nTw4EFVr15dnTt31t69ey3r16xZo8DAQFWuXNkSSlSoUEEzZszQsGHDFBcXZ+k7a9Ys/fLLL1q0aJGlzc/PTytXrrSEFpJ08+ZNrVixQo899phNPem/I3cuK1assOrj6uqqcePGZXpOBw8etGy7Zs0aSdb395/hSmb8/PwUGRlp1bZv3z5duHBBhQsXtuk/efJkm9qHDRtm1efs2bPau3evhg4dqoiIiAyP6+rqqpkzZ+ry5ct21QkAAAAAeSHfg4fExERFR0dr8ODBatOmjdVfe9MVK1ZM3t7eqlatmhYtWqTr169r8+bNlvUuLi7y9vaWn5+fevTooZ49e2rdunUZ7iMgIEDTpk1TcnKytm/fblkfHh6uvXv3auPGjerSpYtKly6t2rVra82aNapUqZL69esns9msXr16KSkpSatXr7ba/9mzZ7Vt2zb169fPqr1EiRLy9va2WgoU+L/LbjQa5e3tLV9fX1WtWlXDhg3Tzp07dfToUc2cOTPb62c2mxUSEqLy5ctr9+7dat26tcqVK6caNWpo4sSJ+vzzz636Dxs2TB9//LGOHDmS4f7urNPT01MGg8GmzR5FihSxXO+FCxfK1dVV69evt6yPiIhQUFCQgoKCrB6Uhw0bpho1amjAgAGSpJ9//lkTJkzQBx98YBXYPPnkk3rsscesHvbXrl0rPz8/PfHEEzb1pP+O3LkULVrUqs9LL72kffv26csvv8zwnB555BHLtl5eXpKs7296W3Z69uypnTt36vfff7e0LVmyRD179pTRaLTp7+7ublP7PwOKyMhItWnTRoMHD9aqVassI37u1KRJE3l7eyssLMyuOgEAAAAgL+R78LBq1SpVrFhRFStWVFBQkCIjI2U2mzPt7+bmJklKTk7OtE+hQoUyXZ+cnKzFixdLkpydnS3tn3zyiZo2barq1atb9S9QoIBeffVVHT9+XIcPH1axYsXUrl07m79YR0ZG6tFHH1XLli2zPmE7BAQEqGXLlnb9BT0mJkbHjh3TqFGjrAKNdEWKFLH6uW7dumrTpo1CQ0Pvuk57OTs7y2g0Wu7Jf//7X+3du1ddunRRly5d9N133+nXX3+VdHtERmRkpHbv3q3FixcrJCREXbt2Vfv27W3226dPH6v7sGTJEvXt2zfXdZYpU0aDBg1SaGio0tLScr2f7Dz66KNq3ry5li5dKkm6fv26Vq1alevazWazIiMjFRQUpICAAFWoUEHR0dE2/ZycnDR9+nTNnTtXf/zxh937T0pKUkJCgtUCAAAAAPbK9+Ah/S/f0u3h8NeuXdM333yTYd/ExESFhobKyclJDRo0yLDPgQMH9Mknn6hx48ZW7XXr1pXJZJKrq6tGjRqlMmXKqEuXLpb1p06dUqVKlTLcZ3r7qVOnJEl9+/bVrl27LA/LZrNZUVFRCgkJkZOTk9W2pUqVkslksiwVK1bM7pJIuh0+xMbGZtvv9OnTlv72CgsL06ZNm7R79267t8mtpKQkTZ06VQkJCZZ7smTJErVs2dIyx0OLFi20ZMkSyzaPPfaYwsPDNWjQIJ0/fz7T1zt69eqlPXv2KDY2Vr/99pu+/fZby+/SP23cuNHqPphMJk2ZMsWm3xtvvKEzZ85o+fLleXD2mevbt6+ioqJkNpv16aefWkapZGTcuHE2te/YscOyfuvWrbp+/bqaN28uSTajSO7UoUMHy2gYe4WFhcnT09Oy+Pn52b0tAAAAAORr8HDy5EkdOHBA3bp1k3T7tYOuXbtaPYRK/xcauLu7a8OGDYqKilLVqlUt69MfKl1dXfXMM8+ofv36mjt3rtU+Vq1apR9//FHr16+Xv7+/PvzwQ7uHxqePwEifhK9Zs2YqVaqU5a/t27ZtU2xsrPr06WOz7e7duxUTE2NZvv76a7uPac+kf/+szR6VK1dW7969s5zP4G51795dJpNJbm5uevfdd/XOO++oZcuWSk1N1dKlS60CgqCgIC1dulSpqamWtj59+sjHx0evvPJKpq93FC9eXK1bt9bSpUsVGRmp1q1bq3jx4hn2bdSokdV9iImJ0ZAhQ2z6PfLIIxo9erQmTJigW7du3eVVyFzr1q117do17dq1K9uRGmPGjLGp/emnn7asj4iIUNeuXS2vaXTv3l379+/XyZMnM9zfzJkztXTpUh0/ftyuWkNDQxUfH29Z7nxFBAAAAACyY/tC+T0UERGhlJQUlSxZ0tJmNpvl7OxsNQHeqlWrVLlyZRUpUkTFihWz2U+jRo20YMECOTs7y9fX1+oVinR+fn4qX768ypcvL5PJpI4dO+r48eOWeQMqVKiQ6YPYzz//LEkqX768pNuvX4SEhCgqKkpvvvmmIiMjVb9+fcv6O5UtW9bmdQd7nDhxQmXLls22X4UKFSz9M/uLeUbefPNNVahQwWYujLwye/ZsNWnSRB4eHlZzM3z99dc6d+6cunbtatU/NTVVmzdvtnpVxWg0ZjjnwZ369u2roUOHSpLef//9TPsVLlxY/v7+dtU+cuRIzZ8/X/Pnz7erf24YjUb16tVLEydO1P79+/XZZ59l2rd48eKZ1n7p0iWtW7dOycnJWrBggaU9NTVVS5YsyXCekPr166t58+YaP368XV8pcXFxkYuLS/YnBQAAAAAZyLcRDykpKVq2bJlmzZpl9Zfcw4cPq3Tp0lZD3f38/FSuXLkMQwfp/x4qS5cunWHo8E8NGjRQYGCgpk2bZmnr1q2btm7dqsOHD1v1TUtL0+zZs1W5cmWr+R/69OmjP/74Q2vXrtXatWttJpW8Gz///LM2bdqkjh07Ztu3Ro0aqly5smbNmpXhvARXrlzJcDs/Pz8NHTpU48ePtxppkFe8vb3l7+9vFTpIt8Ombt262fwFv2fPnpm+HpCVFi1a6NatW7p165blVYO7ZTKZ9J///EfTpk1z6HwGffv21c6dO9WuXTubiS7ttXz5cpUqVUqHDx+2up7h4eFaunSpUlJSMtxuxowZ2rBhg7777ru7OQUAAAAAyFa+jXjYuHGjLl++rH79+tkMpe/UqZMiIiLUpk0bhx1/1KhR6ty5s8aOHauSJUvq1Vdf1eeff662bdtq1qxZevrpp/Xnn39q+vTpOnHihLZu3Wr1OkPZsmX1/PPPa+DAgXJ2dlanTp0yPM5ff/2lmzdvWrUVK1bMEpCkpKTowoULSktL08WLF7Vjxw5NnTpVNWrU0JgxY7I9j/TJGJs0aaL69etr/PjxCggI0LVr17RhwwZt3rxZO3fuzHDb0NBQLV68WGfOnLEZgeAIf//9tzZs2KD169crMDDQal1wcLBat26tv//+W4888ojd+3RyctKJEycs/85MUlKSLly4YNVmNBozfTVj4MCBmj17tlasWGH1WkNeqlSpkv73v/9ZJkzNzNWrV21qd3Nzk4eHhyIiItSpUyeb61m6dGmNGzdOX3zxhdq1a2ezz6pVq6pnz542ryQBAAAAQF7LtxEPERERatKkSYbv73fs2FExMTG6dOmSw47fpk0blSlTxjLqwdXVVdu2bVNwcLDGjx8vf39/tWjRQk5OTtq3b5/q1Kljs49+/frp8uXL6tatW6YPjxUrVpSPj4/V8v3331vWHzt2TD4+PnrsscfUsGFDRUdHKzQ0VLt375bJZLLrXGrXrq1Dhw6pXLlyGjBggCpVqqQXXnhBx44dU3h4eKbbeXl5ady4cTbBiKMsW7ZMhQsXtpn4U7r9uoy7u7s++uijHO/Xw8NDHh4eWfbZtGmTzX2oV69epv2dnZ01ZcoUh1+bYsWKqVChQln2mTBhgk3tY8eO1ffff6/Dhw9nODLG3d1dzZo1y3IUyZQpU7L8ggwAAAAA5AWDmScPADmQkJBw++sWI6JVwCXr0RoAkBuxM1rndwkAAMAO6c8G8fHxWf4xON8/pwkAAAAAAB5cBA//cumvXGS23GvTp0/PtJY7v0jxMFq+fHmm16ZKlSr5XR4AAAAA5It8/ZwmslezZk3FxMTkdxkWgwYNUpcuXTJcl91cBQ+6F154IdOJKO352goAAAAAPIgIHv7lChUqJH9///wuw8LLy0teXl75Xca/kru7u9zd3fO7DAAAAAD4V+FVCwAAAAAA4DAEDwAAAAAAwGEIHgAAAAAAgMMQPAAAAAAAAIcheAAAAAAAAA5D8AAAAAAAAByGz2kCyJWjbzaXh4dHfpcBAAAA4F+OEQ8AAAAAAMBhCB4AAAAAAIDDEDwAAAAAAACHIXgAAAAAAAAOQ/AAAAAAAAAchuABAAAAAAA4DMEDAAAAAABwGGN+FwDg/hQ48WsVcHHL7zIAIEuxM1rndwkAADz0GPEAAAAAAAAchuABAAAAAAA4DMEDAAAAAABwGIIHAAAAAADgMAQPAAAAAADAYQgeAAAAAACAwxA8AAAAAAAAhyF4AAAAAAAADkPwAAAAAAAAHIbgAQAAAAAAOAzBAwAAAAAAcBiCBwAAAAAA4DAEDwAAAAAAwGEIHuz03XffycnJSS1atLBZd+vWLb311luqXr263NzcVLx4cT377LOKjIxUcnKyDAZDlktISIgkyWAwaN26dfrzzz/l7Oysjz/+OMNaXnrpJVWrVk2SNGnSpAz3GRAQYNd5NWzY0LKNi4uLSpYsqbZt22rt2rWZblOxYkUVLFhQ586dkyT973//k7e3t6ZPn27Tt0uXLqpVq5ZSUlKUmJiocePG6fHHH5erq6seeeQRNWzYUBs3brSrVkn65Zdf1KdPH5UqVUouLi4qW7asunfvrkOHDln63HkdTCaTqlevrqioKKv97NixI9P7ceHCBUnW19ZoNKp48eKqX7++wsPDlZSUZHMdR4wYodjY2Gzv96RJk7I8x/R9GI1GyzVOFxcXJ6PRKIPBoNjYWJttmzVrJicnJ+3bt8+qPTU1VXXr1lXHjh2t2uPj4+Xn56c33ngjy5oAAAAAILcIHuy0ZMkSDRs2THv27NHZs2ct7bdu3VLz5s01Y8YMDRw4UN99950OHDigIUOGaO7cuTp27Jji4uIsS3h4uDw8PKza5syZY3WsRx99VK1bt1ZkZKRNHTdu3NDKlSvVr18/S1uVKlWs9hcXF6c9e/bYfW4DBgxQXFycfvnlF61Zs0aVK1dWt27dNHDgQJu+e/bs0c2bN9W5c2fLw3zx4sX1wQcf6M0339SRI0csfT/99FNt2LBBy5Ytk9Fo1KBBg7Ru3TrNmzdPP//8szZt2qSOHTvq4sWLdtV56NAhPfXUUzp16pQWLVqk48eP67PPPlNAQIBGjRpl1TcyMlJxcXE6fPiwunbtqj59+ujrr7+22efJkydtrl2JEiUs69Ov7dmzZ7V9+3Z17txZYWFhqlu3rq5evWqzPz8/P6t9jRo1yub+jB492q7z9fX11bJly6zali5dqpIlS2bY/+zZs9q7d6+GDh2qiIgIq3VOTk5aunSpNm3apOXLl1vahw0bJi8vL02YMMGumgAAAAAgp4z5XcD9IDExUdHR0Tp48KAuXLigqKgoy4NaeHi4du3apUOHDumJJ56wbPP444+rc+fOunXrlgoXLmxp9/T0lMFgkLe3d5bH7Nevn9q1a6fY2FiVKVPG0v7pp5/q5s2bCgoKsrQZjcZs95cVNzc3y/Z+fn6qU6eOAgIC1LdvX3Xp0kVNmjSx9I2IiFCPHj3UoEEDDRkyROPHj5fBYNALL7ygHj16qHfv3jpw4ICuXLmil19+WWFhYapUqZIkacOGDZozZ45atWolSSpTpoyeeuopu2o0m80KCQlR+fLltXv3bhUo8H+ZWY0aNTR8+HCr/kWKFLGc0/jx4zVr1ixt3rxZzZs3t+pXokQJFSlSJNPj3nltfX19VbVqVTVt2lTVq1fXzJkzNXXqVKv+Tk5OVvfCZDLl+v4EBwcrMjJSoaGhlraoqCgFBwdrypQpNv0jIyPVpk0bDR48WLVr11Z4eLjV71758uUVFhamYcOGqVGjRjp48KBWrlypAwcOqGDBgjmuDwAAAADswYgHO6xatUoVK1ZUxYoVFRQUpMjISJnNZknS8uXL1aRJE6vQIZ2zs7PVg19OtGrVSt7e3javCCxZskTt27dXsWLFcrVfewUHB6to0aJWr1xcvXpVq1evVlBQkJo2barExETt2LHDsn7OnDm6dOmSpkyZopdfflmBgYFWgYC3t7e+/PLLDEcKZCcmJkbHjh3TqFGjrEKHdJmFB6mpqYqOjtalS5fk7Oyc4+NmJCAgQC1btszydZS88MILL+jy5cuW0St79uzRpUuX1LZtW5u+ZrNZkZGRCgoKUkBAgCpUqKDo6GibfsOGDVP16tXVu3dvDRw4UBMmTFCNGjWyrCMpKUkJCQlWCwAAAADYi+DBDhEREZYRBi1atNC1a9f0zTffSJJOnz5t93wKOeHk5KTevXsrKirKEnKcOXNGO3futHrNQpKOHDkik8lktfTv3/+ujl+gQAFVqFDBah6BlStXqnz58qpSpYqcnJzUrVs3qyH9Hh4eioyM1PTp07V582ZFRkbKYDBY1n/wwQf67rvvVKxYMdWqVUuvvvqqvv32W7vqOX36tCTZfa27d+8uk8kkFxcXde3aVV5eXhlek1KlSlldt4oVK9q1/4CAgAznWMhLzs7OCgoK0pIlSyTdDp2CgoIyDFC2bt2q69evW0Z0BAUF2bxuId2e/2LBggX65ptv9Oijj+q1117Lto6wsDB5enpaFj8/v7s8MwAAAAAPE4KHbJw8eVIHDhxQt27dJN0eet+1a1fLw6DZbLZ6uM5L/fr102+//aZt27ZJuv3gWapUKatXH6Tbkz3GxMRYLdOmTbvr4//z3O4MYKTbD7dr167VlStXLG3PP/+86tSpo169eql06dJW+6tfv75+/fVXffPNN+rYsaOOHTum5557LsPXBjKqRZLd13r27NmKiYnRli1bVKNGDc2ePVv+/v42/Xbv3m113TKaByKzehx13+/Ur18/rV69WhcuXNDq1avVt2/fDPtFRESoa9euMhpvvz3VvXt37d+/XydPnrTpu2TJErm5uenMmTP6448/sq0hNDRU8fHxluX333+/u5MCAAAA8FAheMhGRESEUlJSVLJkSRmNRhmNRi1YsEBr167V5cuXVaFCBZ04ccIhxy5fvryee+45RUZGKi0tTUuXLlWfPn1sXjUoWLCg/P39rZZHH330ro6dmpqq06dPq2zZspKk48ePa//+/Ro7dqzlOtSpU0c3btzQihUrrLZNX58RZ2dnPffcc3rttde0efNmTZ48WVOmTNGtW7eyrKdChQqSZPe19vb2lr+/vxo1aqTVq1dryJAhOn78uE2/smXLWl23O+fTyMqJEycs18aRAgMDFRAQoO7du6tSpUoKDAy06XPp0iWtW7dO8+fPt1z7kiVLKiUlxRKQpdu7d69mz56tzz//XM8884z69etnCXUy4+LiIg8PD6sFAAAAAOxF8JCFlJQULVu2TLNmzbL6q/jhw4dVunRpLV++XD169NDWrVv1448/Zrh9YmLiXdXQr18/rV27VmvWrNEff/yhPn363NX+7LV06VJdvnzZ8vnFiIgI1a9fX4cPH7a6FmPHjs1wSL+9KleurJSUFN28eTPLfjVq1FDlypU1a9YspaWl2ay/c9TFP/n7+6tjx45WkzTejTu/yHEv9O3bVzt27Mh0tMPy5ctVqlQpm3sTHh6upUuXKiUlRdLtL6IEBwfrpZdeUpMmTfThhx/q4MGDWrRo0T05DwAAAAAPJ75qkYWNGzfq8uXL6tevnzw9Pa3WderUSREREdq3b5+++OILNW7cWFOmTFG9evXk7u6uQ4cOaebMmYqIiMh28r6sdO7cWa+88opeeuklNW7cOMO/yKekpOjChQtWbQaDwe5RD9evX9eFCxeUkpKic+fOae3atZo9e7YGDx6sRo0aKTk5WR999JEmT55s8xf3/v3766233tLhw4dVvXr1LI/TsGFDde/eXTVr1lSxYsV0/PhxjR8/Xo0aNcr2r+gGg0GRkZFq0qSJ6tevr/HjxysgIEDXrl3Thg0btHnzZu3cuTPT7UeNGqXq1avr0KFDqlmzpqX9r7/+sgk9ihUrZplHIf3apqWl6eLFi9qxY4emTp2qGjVqaMyYMVnWnFcGDBigzp07ZzqBZkREhDp16mRzb0qXLq1x48bpiy++ULt27fTaa68pLS1NM2fOlCQ99thjmjVrlkaOHKkWLVrYPdoDAAAAAHKCEQ9ZiIiIUJMmTWxCB0nq2LGj5UsLW7Zs0dixY7Vo0SLVqVNHtWrV0nvvvadXXnklw6HxOeHm5qZu3brp8uXLmf7F+9ixY/Lx8bFa/jm/QlYWL14sHx8flStXTh06dNDx48e1atUqzZ8/X5K0fv16Xbx4UR06dLDZtnz58qpatapdox6aN2+upUuXqlmzZqpUqZKGDRum5s2bZ/j1hYzUrl1bhw4dUrly5TRgwABVqlRJL7zwgo4dO6bw8PAst61ataqaNGli+QxquooVK9pcu++//96yPv3aPvbYY2rYsKGio6MVGhqq3bt3y2Qy2VX33TIajSpevHiGr698//33Onz4cIajL9zd3dWsWTNFRERo586dev/99xUVFWX1pZUBAwaobt26dr1yAQAAAAC5YTDztAEgBxISEm5/3WJEtAq4uOV3OQCQpdgZrfO7BAAAHljpzwbx8fFZjmJnxAMAAAAAAHAYgocHWPrrAJkt/yb3U613a9CgQZme56BBg/K7PAAAAADIU0wu+QCrWbOmYmJi8rsMu9xPtd6tyZMna/To0Rmu41OVAAAAAB40BA8PsEKFCsnf3z+/y7DL/VTr3SpRooRKlCiR32UAAAAAwD3BqxYAAAAAAMBhCB4AAAAAAIDDEDwAAAAAAACHIXgAAAAAAAAOQ/AAAAAAAAAchuABAAAAAAA4DJ/TBJArR99sLg8Pj/wuAwAAAMC/HCMeAAAAAACAwxA8AAAAAAAAhyF4AAAAAAAADkPwAAAAAAAAHIbgAQAAAAAAOAzBAwAAAAAAcBiCBwAAAAAA4DDG/C4AwP0pcOLXKuDilt9lAAAcLHZG6/wuAQBwn2PEAwAAAAAAcBiCBwAAAAAA4DAEDwAAAAAAwGEIHgAAAAAAgMMQPAAAAAAAAIcheAAAAAAAAA5D8AAAAAAAAByG4AEAAAAAADgMwQMAAAAAAHAYggcAAAAAAOAwBA8AAAAAAMBhCB4AAAAAAIDDEDwAAAAAAACHIXjAXQkJCZHBYJDBYJCzs7MeffRRNW3aVEuWLFFaWppN/2bNmsnJyUn79u2TJCUlJalKlSoaOHCgTd+xY8eqdOnSSkhIUGpqqsLCwhQQEKBChQrJy8tLderUUWRkZI7rvHNp0aKFpU+ZMmVkMBi0cuVKm+2rVKkig8GgqKgom/4Gg0Fubm4KDAzUokWLLOujoqJUpEiRLOtaunSpateurcKFC8vd3V3169fXxo0bJUmnTp2Sm5ubPvnkE6tt0tLSVLduXXXo0CHH52YwGFSoUCGVKVNGXbp00bZt2+y6fgAAAACQWwQPuGstWrRQXFycYmNj9dVXX6lRo0YaPny42rRpo5SUFEu/s2fPau/evRo6dKgiIiIkSS4uLlq2bJmioqK0adMmS999+/Zp9uzZioqKkoeHhyZNmqTw8HBNmTJFx48f1/bt2zVgwABdvnw5x3XeuaxYscKqj5+fn02YsW/fPl24cEGFCxe22efkyZMVFxenn376Se3bt9egQYO0atUqu+oZPXq0XnrpJXXp0kWHDx/WgQMH9Nxzz6ldu3aaN2+eKlSooBkzZmjYsGGKi4uzbDdr1iz98ssvViGHPeeWXuvJkye1bNkyFSlSRE2aNNG0adPsqhcAAAAAcsOY3wXg/ufi4iJvb29JUsmSJfXkk0+qTp06aty4saKiotS/f39JUmRkpNq0aaPBgwerdu3aCg8PV+HChfXUU0/p9ddfV//+/XX06FG5urqqT58+GjJkiBo1aiRJ2rBhg15++WV17tzZctzq1avnus7M9OzZU7Nnz9bvv/8uPz8/SdKSJUvUs2dPLVu2zKa/u7u7ZZ9Tp05VdHS01q1bp65du2Z5nH379mnWrFl67733NGzYMEv7tGnTdPPmTY0cOVLt2rXTsGHD9Pnnn2vAgAHauHGjfv75Z02YMEErVqxQiRIlcnRud9b62GOPqX79+vLx8dGECRPUqVMnVaxYMcPtkpKSlJSUZPk5ISEhy+MAAAAAwJ0Y8QCHeP7551W9enWtXbtWkmQ2mxUZGamgoCAFBASoQoUKio6OtvR//fXX5ePjo1deeUVvvPGGJCksLMyy3tvbW9u2bdPff//t0LofffRRNW/eXEuXLpUkXb9+XatWrVLfvn3t2t7V1VXJycnZ9luxYoVMJpNeeuklm3WjRo1ScnKy1qxZI4PBoMjISO3evVuLFy9WSEiIunbtqvbt2+fovDIzfPhwmc1mff7555n2CQsLk6enp2VJD2QAAAAAwB4ED3CYgIAAxcbGSpK2bt2q69evq3nz5pKkoKAgy+sWkmQ0GrVs2TKtXr1ac+fO1bJly1SoUCHL+nfffVd///23vL29Va1aNQ0aNEhfffVVjurZuHGjTCaT1TJlyhSbfn379lVUVJTMZrM+/fRTlStXTjVq1Mhy3ykpKYqKitKRI0fUuHHjbGs5deqUypUrp4IFC9qs8/X1laenp06dOiXp9uiE8PBwDRo0SOfPn9ecOXNyfW7/5OXlpRIlSljuU0ZCQ0MVHx9vWX7//fds9wsAAAAA6XjVAg5jNptlMBgkSREREeratauMxtu/ct27d9eYMWN08uRJyxD/SpUqqWPHjrpy5Ypq1aplta/KlSvr6NGj+v7777Vnzx7t2rVLbdu2VUhIiD788EO76mnUqJEWLFhg1ebl5WXTr3Xr1nrppZe0a9cuLVmyJMvRDuPGjdMbb7yhpKQkFSxYUGPGjMlwFENO3XntJKlPnz76z3/+o1deeUWenp42/e09N3uO9U8uLi5ycXGxs3IAAAAAsEbwAIc5ceKEypYtq0uXLmndunVKTk62ejhOTU3VkiVLNHPmTEub0Wi0hBP/VKBAAdWqVUu1atXSq6++qo8//li9evXS66+/rrJly2ZbT+HCheXv759tP6PRqF69emnixInav3+/Pvvss0z7jhkzRiEhIXJzc5OPj0+WD/B3qlChgvbs2aNbt27ZjHo4f/68EhISVL58eZu6Mrs29p7bP128eFF///23XdcPAAAAAHKDVy3gENu2bdORI0fUsWNHLV++XKVKldLhw4cVExNjWcLDw7V06VKrL1/kROXKlSVJiYmJeVm6pNuvW+zcuVPt2rVT0aJFM+1XvHhx+fv7y9fX1+7QQZK6deuma9euWX2ZIt0777wjZ2dndezYMVe158ScOXNUoECBPJszAgAAAAD+iREPuGtJSUm6cOGCUlNT9eeff2rTpk0KCwtTmzZt1Lt3bz311FPq1KmTAgMDrbYrXbq0xo0bpy+++ELt2rXL8hidOnXSs88+q7p168rb21tnzpxRaGioKlSooICAgBzVeSej0ajixYvb9K1UqZL+97//yc3Nza59ZyY1NVUxMTFWbQULFtQzzzyj4cOHa8yYMbp165bat2+v5ORkffzxx5ozZ47Cw8NzNImjPed29epVXbhwQcnJyTpz5ow+/vhjffjhhwoLC8vVaAkAAAAAsAfBA+7apk2b5OPjI6PRqKJFi6p69ep67733FBwcrB9//FGHDx/W4sWLbbZzd3dXs2bNFBERkW3w0Lx5c61YsUJhYWGKj4+Xt7e3nn/+eU2aNCnT1w8yq/NOFStW1M8//5xh/2LFitm136xcu3ZNTzzxhFVb6dKlFRsbq/DwcFWrVk0LFizQf/7zHxkMBj355JNat26d2rZtm6Pj2HNuEyZM0IQJE1SwYEF5e3urTp06+uabbyyfLAUAAAAARzCYzWZzfhcB4P6RkJBw+7OaI6JVwOXuRoQAAP79Yme0zu8SAAD/UunPBvHx8fLw8Mi0H3M8AAAAAAAAhyF4wH3v7NmzMplMmS5nz57N7xIBAAAA4KHFHA+47/n6+tpM4PjP9QAAAACA/EHwgPue0WjkqwwAAAAA8C/FqxYAAAAAAMBhCB4AAAAAAIDDEDwAAAAAAACHIXgAAAAAAAAOQ/AAAAAAAAAchuABAAAAAAA4DJ/TBJArR99sLg8Pj/wuAwAAAMC/HCMeAAAAAACAwxA8AAAAAAAAhyF4AAAAAAAADkPwAAAAAAAAHIbgAQAAAAAAOAzBAwAAAAAAcBiCBwAAAAAA4DDG/C4AwP0pcOLXKuDilt9lAAAcLHZG6/wuAQBwn2PEAwAAAAAAcBiCBwAAAAAA4DAEDwAAAAAAwGEIHgAAAAAAgMMQPAAAAAAAAIcheAAAAAAAAA5D8AAAAAAAAByG4AEAAAAAADgMwQMAAAAAAHAYggcAAAAAAOAwBA8AAAAAAMBhCB4AAAAAAIDDEDzgvhcSEiKDwSCDwSBnZ2c9+uijatq0qZYsWaK0tDSb/s2aNZOTk5P27dsnSUpKSlKVKlU0cOBAm75jx45V6dKllZCQoNTUVIWFhSkgIECFChWSl5eX6tSpo8jIyBzXaTAYVKxYMbVo0UI//fSTVT+DwaB169ZZ/Zy+mEwmVa9eXVFRURnuM6MlvV/79u1tatqxY4cMBoOuXLli1zkAAAAAQE4RPOCB0KJFC8XFxSk2NlZfffWVGjVqpOHDh6tNmzZKSUmx9Dt79qz27t2roUOHKiIiQpLk4uKiZcuWKSoqSps2bbL03bdvn2bPnq2oqCh5eHho0qRJCg8P15QpU3T8+HFt375dAwYM0OXLl3NcZ1xcnL755hsZjUa1adMm2+0iIyMVFxenw4cPq2vXrurTp4++/vprzZkzx7K/uLg4q753tgEAAABAfjHmdwFAXnBxcZG3t7ckqWTJknryySdVp04dNW7cWFFRUerfv7+k2w/lbdq00eDBg1W7dm2Fh4ercOHCeuqpp/T666+rf//+Onr0qFxdXdWnTx8NGTJEjRo1kiRt2LBBL7/8sjp37mw5bvXq1XNdp7e3t8aNG6f69evr77//1iOPPJLpdkWKFLFsN378eM2aNUubN29W8+bN5enpmWlfAAAAAMhvjHjAA+v5559X9erVtXbtWkmS2WxWZGSkgoKCFBAQoAoVKig6OtrS//XXX5ePj49eeeUVvfHGG5KksLAwy3pvb29t27ZNf//9d57Ud+3aNS1fvlz+/v4qVqyYXdukpqYqOjpaly5dkrOzc57UkZ2kpCQlJCRYLQAAAABgr1yPePjoo4+0cOFCnTlzRnv37lXp0qUVHh6usmXLql27dnlZI5BrAQEBljkUtm7dquvXr6t58+aSpKCgIEVERKhPnz6SJKPRqGXLlunJJ59UWlqa9uzZo0KFCln29e6776pTp07y9vZWlSpVVLduXbVr104tW7a0u56NGzfKZDJJkhITE+Xj46ONGzeqQIGsM8Du3bvLyclJN2/eVGpqqry8vCyjOHJz7HSpqanZbhcWFqY333wzR8cCAAAAgHS5GvGwYMECjRw5Uq1atdKVK1csDy9FihRReHh4XtYH3BWz2WyZYDEiIkJdu3aV0Xg7b+vevbv279+vkydPWvpXqlRJHTt2VNOmTVWrVi2rfVWuXFlHjx7Vvn371KdPH/35559q27ZtjgKARo0aKSYmRjExMdq/f7+aNWumli1b6rfffstyu9mzZysmJkZbtmxRjRo1NHv2bPn7+9t93H8eO3358MMPs90uNDRU8fHxluX333/P0XEBAAAAPNxyFTzMnTtXixcv1uuvvy4nJydLe82aNXXkyJE8Kw64WydOnFDZsmV16dIlrVu3TvPnz5fRaJTRaFTJkiWVkpKiJUuWWG2Tvj4jBQoUUK1atfTqq6/qs88+U1RUlCIiInTmzBm76ilcuLD8/f3l7++v2rVrKyIiQomJiVq8eHGW23l7e8vf31+NGjXS6tWrNWTIEB0/fty+i5DBsdOXkiVLZrudi4uLPDw8rBYAAAAAsFeugoczZ87oiSeesGl3cXFRYmLiXRcF5IVt27bpyJEj6tixo5YvX65SpUrp8OHDVn/xDw8P19KlS62+fJETlStXlqRc/94bDAYVKFBAN27csHsbf39/dezYUaGhobk6JgAAAADcS7ma46Fs2bKKiYlR6dKlrdq/+uory4MYcC8lJSXpwoULSk1N1Z9//qlNmzYpLCxMbdq0Ue/evfXUU0+pU6dOCgwMtNqudOnSGjdunL744ots5ybp1KmTnn32WdWtW1fe3t46c+aMQkNDVaFCBQUEBOSoTkm6fPmy5s2bp2vXrqlt27Y5Ot9Ro0apevXqOnTokGrWrJmjbQEAAADgXspV8DBmzBgNGTJEN2/elNls1oEDB7RixQqFhYXZ9c44kNc2bdokHx8fGY1GFS1aVNWrV9d7772n4OBg/fjjjzp8+HCGrzO4u7urWbNmioiIyDZ4aN68ueX3PD4+Xt7e3nr++ec1adKkTF/NyKzO9GMHBARo9erVatiwYY7Ot2rVqmrSpIkmTJigL7/8MkfbAgAAAMC9ZDCbzebcbLh48WJNnTrVMtFcyZIlNWnSJPXr1y9PCwTw75KQkCBPT0/5jYhWARe3/C4HAOBgsTNa53cJAIB/qfRng/j4+CzngsvxiIeUlBQtX75cbdu21YABA/S///1PaWlpKlGixF0VDAAAAAAAHjw5nlzSaDRq8ODBSkpKkiQVL16c0AEPvbNnz8pkMmW6nD17Nr9LBAAAAIB8kas5Hp5++mn9+OOPNpNLAg8rX19fxcTEZLkeAAAAAB5GuQoeXn75ZY0aNUp//PGHnnrqKRUuXNhqfbVq1fKkOOB+YTQa5e/vn99lAAAAAMC/Tq6Ch65du0qSXnnlFUubwWCQ2WyWwWBQampq3lQHAAAAAADua7kKHs6cOZPXdQAAAAAAgAdQroIH5nYAAAAAAAD2yFXwsGzZsizX9+7dO1fFAAAAAACAB0uugofhw4db/ZycnKzr16+rYMGCcnNzI3gAAAAAAACSpAK52ejy5ctWy7Vr13Ty5EnVq1dPK1asyOsaAQAAAADAfcpgNpvNebWzQ4cOKSgoSD///HNe7RLAv0xCQoI8PT0VHx8vDw+P/C4HAAAAQD6x99kgVyMeMuPk5KTz58/n5S4BAAAAAMB9LFdzPKxfv97qZ7PZrLi4OM2bN0/PPvtsnhQGAAAAAADuf7kKHtq3b2/1s8Fg0COPPKLnn39es2bNyou6AAAAAADAAyBXwUNaWlpe1wEAAAAAAB5AuZrjYfLkybp+/bpN+40bNzR58uS7LgoAAAAAADwYcvVVCycnJ8XFxalEiRJW7RcvXlSJEiWUmpqaZwUC+HfhqxYAAAAAJAd/1cJsNstgMNi0Hz58WF5eXrnZJQAAAAAAeADlaI6HokWLymAwyGAwqEKFClbhQ2pqqq5du6ZBgwbleZEAAAAAAOD+lKPgITw8XGazWX379tWbb74pT09Py7qCBQuqTJkyeuaZZ/K8SAD/PoETv1YBF7f8LgMA8ACIndE6v0sAADhQjoKH4OBgSVLZsmVVt25dOTs7O6QoAAAAAADwYMjV5zQbNGhg+feNGzeUnJxstZ4J5wAAAAAAgJTLySWvX7+uoUOHqkSJEjKZTCpatKjVAgAAAAAAIOUyeBgzZoy2bdum+fPny8XFRR9++KHefPNN+fr6atmyZXldIwAAAAAAuE/l6lWLDRs2aNmyZWrYsKH69u2r5557Tv7+/ipdurSWL1+unj175nWdAAAAAADgPpSrEQ+XLl1S2bJlJd2ez+HSpUuSpHr16mnXrl15Vx0AAAAAALiv5Sp4ePzxxxUbGytJqly5sqKjoyXdHglRpEiRvKoNAAAAAADc53IVPPTp00eHDx+WJIWGhlrmenj11Vc1ZsyYPC0QAAAAAADcv3I1x8Orr75q+XejRo30888/69ChQypXrpyqV6+eZ8UBAAAAAID7W66ChzvdvHlTjz32mB577LG8qAcAAAAAADxAcvWqRWpqqqZMmaKSJUvKZDLp119/lST95z//UURERJ4WCAAAAAAA7l+5Ch6mTZumqKgovfXWWypYsKClvWrVqvrwww/zrDgAAAAAAHB/y1XwsGzZMn3wwQfq2bOnnJycLO3VqlXTzz//nGfF4eHz3XffycnJSS1atLBqj42NlcFgkNFo1Llz56zWxcXFyWg0ymAwKDY2VpMmTZLBYMhySf8qS3b++OMPFSxYUAEBARmuNxgMcnV11W+//WbV3r59e4WEhFh+DgkJkcFg0IwZM6z6rVu3TgaDwfJzVFRUpl+GKVKkiKKioqyOvW7dOkVFRWV7vlOmTJGPj4/l07fpDh8+rIIFC+rzzz+342oAAAAAQM7lKng4d+6c/P39bdrT0tKUnJx810Xh4bVkyRINGzZMe/bs0dmzZ23W+/r6atmyZVZtS5cuVcmSJS0/jx49WnFxcZalVKlSmjx5slWbn5+fXfVERUWpS5cuun79ur799tsM+xgMBk2YMCHbfbm6umrmzJm6fPmyXce2V9euXa3O7ZlnntGAAQOs2saNGyc/Pz8NGTLEsl1ycrJCQkLUo0cPtWvXLk9rAgAAAIB0uQoeqlSpot27d9u0r169Wk888cRdF4WHU2JioqKjozV48GC1adPG6q/76YKDgxUZGWnVFhUVpeDgYMvPJpNJ3t7elsXJyUnu7u42bdkxm82KjIxUr1691KNHj0znLxk2bJg+/vhjHTlyJMv9NWnSRN7e3goLC8v22DlRqFAhq3MrWLCg3NzcbNqWLVumzz//XJ9++qmk269MXbp0Se+9916e1gMAAAAAd8pV8DBx4kQNHTpUM2fOVFpamtauXasBAwZo+vTpdv3lF8jIqlWrVLFiRVWsWFFBQUGKjIyU2Wy26vPCCy/o8uXL2rNnjyRpz549unTpktq2bZvn9Wzfvl3Xr19XkyZN1KtXL0VHR+vq1as2/erWras2bdooNDQ0y/05OTlp+vTpmjt3rv744488rzc7AQEBmj59ugYPHqyvv/5aYWFhioyMlIeHR5bbJSUlKSEhwWoBAAAAAHvlKHj49ddfZTab1bZtW61atUpffvmlZZj5iRMntGHDBjVt2tRRteIBFxERoaCgIElSixYtdO3aNX3zzTdWfZydnRUUFKQlS5ZIuv1qRlBQkJydnR1ST7du3eTk5KQqVarI399fq1atyrBvWFiYNm3alOFIoDt16NBBNWrU0MSJE/O8XnsMHz5cgYGBatWqlQYPHqznn38+223CwsLk6elpWex9TQUAAAAApBwGD+XLl9fff/8tSWrevLm8vb31yy+/6Pr169qzZ4+aNWvmkCLx4Dt58qQOHDigbt26SZKMRqO6du1qCRju1K9fP61evVoXLlzQ6tWr1bdv3zyv58qVK1q7dq0lCJFkFXj8U+XKldW7d2+NGzcu233PnDlTS5cu1fHjx/OsXnsZDAa9/vrrSktL0xtvvGHXNqGhoYqPj7csv//+u4OrBAAAAPAgMeak8z+HvX/11Vd5/r46Hk4RERFKSUmxmiTSbDbL2dnZZjLGwMBABQQEqHv37qpUqZICAwMVExOTp/V88sknunnzpp5++mmretLS0nT8+HFVrlzZZps333xTFSpU0Lp167Lcd/369dW8eXONHz/e6ssXkuTh4aFr164pNTXVah6K1NRUXbt2TZ6ennd1XtLtUOfO/5sdFxcXubi43PVxAQAAADyccjXHQ7p/BhFAbqSkpGjZsmWaNWuWYmJiLMvhw4dVunRpLV++3Gabvn37aseOHQ4Z7SDdDkJGjRplU0+jRo0yHfXg5+enoUOHavz48UpNTc1y/zNmzNCGDRv03XffWbUHBAQoNTVVP/74o1X7Dz/8oNTUVFWsWPHuTgwAAAAA7rEcBQ8Gg0EGg8GmDbgbGzdu1OXLl9WvXz8FBgZaLZ06dcrwaxIDBgzQ33//rf79++d5PTExMfrhhx/Uv39/m3q6d++uZcuWZfrZ2NDQUJ0/f15bt27N8hhVq1ZVz549NXfuXKv2ypUrq2XLlurbt6+2bt2qM2fOaOvWrerXr59atmyZ4UgLAAAAAPg3y1HwYDabFRISohdffFEvvviibt68qUGDBll+Tl+AnIiIiFCTJk0yfI2gY8eOiomJ0aVLl6zajUajihcvbvfrAjmtp3LlygoICLBZ1759e126dEkbNmzIcFsvLy+NGzdON2/ezPY4U6ZMyXDU0MqVK9WkSRMNHjxYlStX1uDBg9W4cWOtWLEi5ycDAAAAAPnMYM7B+xJ9+vSxq19kZGSuCwLw75aQkHD76xYjolXAxS2/ywEAPABiZ7TO7xIAALmQ/mwQHx8vDw+PTPvl6M/FBAoAAAAAACAn7mpySeB+ZTKZMl12796d3+UBAAAAwAMj71+QB+4DWX1+885PegIAAAAA7g7BAx5K/v7++V0CAAAAADwUeNUCAAAAAAA4DMEDAAAAAABwGIIHAAAAAADgMAQPAAAAAADAYQgeAAAAAACAwxA8AAAAAAAAh+FzmgBy5eibzeXh4ZHfZQAAAAD4l2PEAwAAAAAAcBiCBwAAAAAA4DAEDwAAAAAAwGEIHgAAAAAAgMMQPAAAAAAAAIcheAAAAAAAAA5D8AAAAAAAAByG4AEAAAAAADiMMb8LAHB/Cpz4tQq4uOV3GQAAPJBiZ7TO7xIAIM8w4gEAAAAAADgMwQMAAAAAAHAYggcAAAAAAOAwBA8AAAAAAMBhCB4AAAAAAIDDEDwAAAAAAACHIXgAAAAAAAAOQ/AAAAAAAAAchuABAAAAAAA4DMEDAAAAAABwGIIHAAAAAADgMAQPAAAAAADAYQgecM+EhITIYDDIYDDI2dlZjz/+uEaPHq3ExERLn4EDB8rJyUkrV66UJJnNZjVp0kTNmze32d/8+fPl6emps2fPaseOHTIYDCpatKhu3rxp1e/AgQOW46ZL75/RcuHCBUnSpEmTZDAYNGjQIKv9xcTEyGAwKDY21tInqyU2NjbL63LnPgoUKCBfX1/17NlTv//+e4b9K1asqIIFC+rcuXPZnkv6EhUVZdc5AwAAAEBeI3jAPdWiRQvFxcXp119/1dSpUzV//nyNHj1aknT9+nWtWrVKY8aMUUREhCTJYDAoMjJS+/fv16JFiyz7OXPmjMaNG6c5c+boscces7S7u7vrs88+szrmkiVLrPrc6eTJk4qLi7NaSpQoYVnv6uqqiIgInTp1KsPtR48ebbVtqVKlNHnyZKs2Pz+/bK9LlSpVFBcXpz/++EOrVq3SkSNH1KVLF5t+e/bs0c2bN9W5c2dFRUVJkurWrWt1vC5duliuc/rStWtXu88ZAAAAAPISwQPuKRcXF3l7e8vPz089evRQz549tW7dOknS6tWrVblyZYWGhurbb7+1jBTw8/PTnDlzNHr0aJ05c0Zms1n9+vVT48aNFRISYrX/4OBgLVmyxPLzjRs3tHLlSgUHB2dYT4kSJeTt7W21FCjwf/+xqFixoho1aqQ33ngjw+1NJpPVtk5OTnJ3d7dpy47RaJS3t7d8fX313HPPacCAAdq3b58SEhKs+kVERKhHjx7q1auXlixZIrPZrIIFC1odr1ChQpbrfGebvecMAAAAAHmJpw3kq0KFCik5OVnS7YfqoKAgeXp6qlWrVoqMjLT0Cw4OVuPGjdWnTx/NmzdPR48e1QcffGCzv169emn37t06e/asJGnNmjUqU6aMnnzyyVzXOGPGDK1Zs0YHDx7M9T5y4sKFC1q7dq2cnJysQourV69q9erVCgoKUtOmTZWYmKgdO3Y4vJ6kpCQlJCRYLQAAAABgL4IH5JsDBw7ok08+UePGjXX69Gnt27fP8kpAUFCQIiMjlZaWZun/wQcf6Pjx4xoxYoQWLVqU4esBJUqUUMuWLS2vISxZskR9+/bNtIZSpUrJZDJZlooVK9r0efLJJ9WlSxe99tprd3nGmTty5IhMJpPc3Nzk4+OjHTt2aMiQISpcuLClz8qVK1W+fHlVqVJFTk5O6tatm+WVlJyw55zvFBYWJk9PT8tiz6sjAAAAAJDOmN8F4OGyceNGmUwmpaSkKDk5We3atdPcuXP17rvvqnnz5ipevLgkqVWrVurXr5+2bt2qZs2aSbodKgwcOFDr1q1Thw4dMj1G3759NXz4cAUFBWnv3r1avXq1du/enWHf3bt3y93d3fKz0ZjxfySmTp2qSpUqafPmzQ6ZD6FixYpav369kpKS9Pnnn2v16tWaNm2aVZ/0ESHpgoKCVL9+fV25ckVFihSx+1j2nnO60NBQjRw50vJzQkIC4QMAAAAAuxE84J5q1KiRFixYIGdnZ/n6+srZ2VmpqalatmyZLly4YPUQnJqaqoiICEvwIN1+SM7uQblVq1Z66aWX1K9fP7Vt21bFihXLtG/ZsmXtemgvV66cBgwYoNdeey1XowyyU7BgQfn7+0u6PdHk6dOnNXjwYH300UeSpOPHj2v//v06ePCgxo0bZ9kuNTVVK1as0ODBg+0+lr3nnM7FxUUuLi529wcAAACAOxE84J4qXLiw5QE73ZdffqmrV6/qxx9/tJrT4Oeff1bPnj118eLFLMODf3JyclKvXr301ltv6auvvsqz2idMmKBy5cpZPvXpSP/5z39UoUIFvfrqq3ryyScVERGh+vXr6/3337fq99FHHykiIiJHwQMAAAAA3EvM8YB8FxERodatW6t69eoKDAy0LB07dtQjjzyijz/+OMf7nDJliv7++281b948y35//fWXLly4YLWkT3b5T48++qhGjhyp9957L8f15NTjjz+udu3aacKECUpOTtZHH32k7t27W12fwMBA9e/fX99//70OHz5s975zcs4AAAAAcLcIHpCv/vzzT33xxRfq2LGjzTqDwaAXX3wxV682FCxYUMWLF5fBYMiyX8WKFeXj42O1fP/995n2HzNmjEwmU47ryY1Ro0bpiy++0LvvvquLFy9mOK9F+fLlVbVq1Rxdo5yeMwAAAADcDYPZbDbndxEA7h8JCQm3v24xIloFXNzyuxwAAB5IsTNa53cJAJCt9GeD+Ph4eXh4ZNqPEQ8AAAAAAMBhCB4ABzOZTJkumX3mEwAAAAAeFHzVAnCwmJiYTNeVLFny3hUCAAAAAPmA4AFwsH9+PhQAAAAAHia8agEAAAAAAByG4AEAAAAAADgMwQMAAAAAAHAYggcAAAAAAOAwBA8AAAAAAMBhCB4AAAAAAIDD8DlNALly9M3m8vDwyO8yAAAAAPzLMeIBAAAAAAA4DMEDAAAAAABwGIIHAAAAAADgMAQPAAAAAADAYQgeAAAAAACAwxA8AAAAAAAAhyF4AAAAAAAADkPwAAAAAAAAHMaY3wUAuD8FTvxaBVzc8rsMAAAAIFuxM1rndwkPNUY8AAAAAAAAhyF4AAAAAAAADkPwAAAAAAAAHIbgAQAAAAAAOAzBAwAAAAAAcBiCBwAAAAAA4DAEDwAAAAAAwGEIHgAAAAAAgMMQPAAAAAAAAIcheAAAAAAAAA5D8AAAAAAAAByG4AEAAAAAADgMwQPuiZCQEBkMBpulRYsWOn/+vLy8vPTee+9ZbbN//345Oztry5YtGW575xISEpJtDdu3b1ejRo3k5eUlNzc3lS9fXsHBwUpJSbH0SU1N1ezZs1WtWjW5urqqSJEiatmypb799lurfU2aNEk1atSwOcaVK1dkMBi0Y8cOSVJsbKxVnZ6enqpTp442bNhgs+2aNWvUsGFDeXp6ymQyqVq1apo8ebIuXbokSYqKisrw3F1dXbM9d0lasGCBqlWrJg8PD3l4eOiZZ57RV199Zde2AAAAAJBbBA+4Z1q0aKG4uDirZcWKFfL19dV7772n0NBQnT59WpJ048YNBQcHq3///mratKnVNuHh4fLw8LBqmzNnTpbHPnbsmFq2bKlatWpp165dOnLkiObOnStnZ2elpaVJksxms7p166bJkyfrlVde0YkTJ7Rz5075+fmpYcOGWrduXa7PfevWrYqLi9P+/ftVu3ZtdezYUUePHrWsf/3119W1a1fVqlVLX331lY4ePapZs2bp8OHD+uijjyz9/nnecXFx+u233+yqoVSpUpoxY4YOHTqkQ4cO6fnnn1e7du107NixXJ8XAAAAAGTHmN8F4OHh4uIib2/vDNcFBQVp7dq1CgkJ0e7duxUaGqpbt27p7bffliSr7Tw9PWUwGDLdV0a2bNkiHx8fvfXWW5a2cuXKqUWLFpafo6Oj9emnn2r9+vVq27atpf2DDz7QxYsXLSFI4cKF7T5uumLFisnb21ve3t6aNm2a5s6dq+3btyswMFAHDhzQ9OnTFR4eruHDh1u2KVOmjJo2baorV65Y2nJ63ne685wkadq0aVqwYIH27dunKlWq5GqfAAAAAJAdRjzgX2PhwoU6ffq0evbsqXnz5ikqKkomkylP9u3t7a24uDjt2rUr0z6ffPKJKlSoYPOALkmjRo3SxYsXtWXLlruqIzk5WYsXL5YkOTs7S5KWL18uk8mkl19+OcNtihQpclfHzEhqaqpWrlypxMREPfPMM1n2TUpKUkJCgtUCAAAAAPYieMA9s3HjRplMJqtlypQplvUlSpTQlClTtHLlSg0cOFD169fPs2N37txZ3bt3V4MGDeTj46MOHTpo3rx5Vg/Rp06dUqVKlTLcPr391KlTuTp+3bp1ZTKZ5OrqqlGjRqlMmTLq0qWLJOn06dN6/PHHLUFEVuLj422uYbNmzeyu48iRIzKZTHJxcdGgQYP02WefqXLlylluExYWJk9PT8vi5+dn9/EAAAAAgFctcM80atRICxYssGrz8vKy/Ds1NVVLly6Vm5ub9u3bp5SUFBmNefMr6uTkpMjISE2dOlXbtm3Tvn37NG3aNM2cOVMHDhyQj4+PXfsxGAy5Ov6qVasUEBCgU6dOacSIEVq4cKHl3M1ms937dXd31w8//GDVVqhQIbvrqFixomJiYnTlyhWtWbNGwcHB2rlzZ5bhQ2hoqEaOHGn5OSEhgfABAAAAgN0Y8YB7pnDhwvL397da7gwe3nnnHZ0+fVoHDx7U+fPnNX369DyvoWTJkurVq5fef/99HT9+XDdv3tTChQslSRUqVNDx48cz3O7EiROSpPLly0u6PcljfHy8Tb/0+Rg8PT2t2v38/FS+fHm1bt1aH374obp27aq//vrLctz//ve/Sk5Ozrb+AgUK2FzDkiVL2nfykgoWLCh/f3/VrFlTYWFhql69erYTc7q4uFi+hJG+AAAAAIC9CB7wr3Ds2DFNnDhRCxYsUOXKlbVw4UJNnTpVP/30k8OOWbRoUfn4+CgxMVGS1K1bN50+fTrDT13OmjVLxYoVU9OmTSVJAQEB+uOPP3ThwgWrfgcPHrSEA5lp0KCBAgMDNW3aNElSjx49dO3aNc2fPz/D/ndOLpnXzGazkpKSHLZ/AAAAAOBVC9wzSUlJNg/qRqNRRYoUUXBwsDp06KBOnTpJktq3b6/OnTsrJCREBw4cuOtXLhYtWqSYmBh16NBB5cqV082bN7Vs2TIdO3ZMc+fOlXQ7eFi9erWCg4P19ttvq3HjxkpISND777+v9evXa/Xq1ZYvWjRr1kyVKlVSt27dNG3aNPn6+uqnn37S6NGjNWjQILm7u2dZz6hRo9S5c2eNHTtWTz/9tMaOHatRo0bp3Llz6tChg3x9ffXLL79o4cKFqlevnuVrF2az2eYaSrfnxyhQIOsccfz48WrZsqX8/Px09epVrVy5Ujt27NCmTZtyc0kBAAAAwC4ED7hnNm3aZDOXQsWKFdWjRw+dO3dOX3/9tdW6uXPnqkqVKpo+fbomTJhwV8euXbu29uzZo0GDBun8+fMymUyqUqWK1q1bpwYNGki6PX9DdHS05syZo9mzZ2vIkCFycXHRM888o+3bt6tevXqW/RmNRm3evFnjx49Xz5499ddff6l06dLq37+/xo4dm209bdq0UZkyZTRt2jTNnz9fM2fO1FNPPaX3339fCxcuVFpamsqVK6dOnTopODjYsl1CQkKG81HExcVl+5nNP//8U7169VJcXJw8PT1VrVo1bdq0yTKKAwAAAAAcwWA2m835XQSA+0dCQsLtr1uMiFYBF7f8LgcAAADIVuyM1vldwgMp/dkgPj4+y7ngmOMBAAAAAAA4DMEDHgjTp0+XyWTKcGnZsmV+l+dwZ8+ezfT8TSaTzp49m98lAgAAAHhIMccDHgiDBg1Sly5dMlxXqFChe1zNvefr66uYmJgs1wMAAABAfiB4wAPBy8tLXl5e+V1GvjEajVl+whMAAAAA8guvWgAAAAAAAIcheAAAAAAAAA5D8AAAAAAAAByG4AEAAAAAADgMwQMAAAAAAHAYggcAAAAAAOAwfE4TQK4cfbO5PDw88rsMAAAAAP9yjHgAAAAAAAAOQ/AAAAAAAAAchuABAAAAAAA4DMEDAAAAAABwGIIHAAAAAADgMAQPAAAAAADAYQgeAAAAAACAwxA8AAAAAAAAhzHmdwEA7k+BE79WARe3/C4DAAAAeKDEzmid3yXkOUY8AAAAAAAAhyF4AAAAAAAADkPwAAAAAAAAHIbgAQAAAAAAOAzBAwAAAAAAcBiCBwAAAAAA4DAEDwAAAAAAwGEIHgAAAAAAgMMQPAAAAAAAAIcheAAAAAAAAA5D8AAAAAAAAByG4AEAAAAAADgMwQPuiZCQEBkMBpulRYsWOn/+vLy8vPTee+9ZbbN//345Oztry5YtGW575xISEpJtDdu3b1ejRo3k5eUlNzc3lS9fXsHBwUpJSbH0SU1N1ezZs1WtWjW5urqqSJEiatmypb799lurfU2aNEk1atSwOcaVK1dkMBi0Y8cOSVJsbKxVnZ6enqpTp442bNhgs+2aNWvUsGFDeXp6ymQyqVq1apo8ebIuXbokSYqKisrw3F1dXbM9d0kKCwtTrVq15O7urhIlSqh9+/Y6efKkXdsCAAAAQG4RPOCeadGiheLi4qyWFStWyNfXV++9955CQ0N1+vRpSdKNGzcUHBys/v37q2nTplbbhIeHy8PDw6ptzpw5WR772LFjatmypWrVqqVdu3bpyJEjmjt3rpydnZWWliZJMpvN6tatmyZPnqxXXnlFJ06c0M6dO+Xn56eGDRtq3bp1uT73rVu3Ki4uTvv371ft2rXVsWNHHT161LL+9ddfV9euXVWrVi199dVXOnr0qGbNmqXDhw/ro48+svT753nHxcXpt99+s6uGnTt3asiQIdq3b5+2bNmilJQUNWvWTImJibk+LwAAAADIjjG/C8DDw8XFRd7e3hmuCwoK0tq1axUSEqLdu3crNDRUt27d0ttvvy1JVtt5enrKYDBkuq+MbNmyRT4+PnrrrbcsbeXKlVOLFi0sP0dHR+vTTz/V+vXr1bZtW0v7Bx98oIsXL1pCkMKFC9t93HTFihWTt7e3vL29NW3aNM2dO1fbt29XYGCgDhw4oOnTpys8PFzDhw+3bFOmTBk1bdpUV65csbTl9LzvtGnTJqufIyMjVaJECX3//feqX79+rvYJAAAAANlhxAP+NRYuXKjTp0+rZ8+emjdvnqKiomQymfJk397e3oqLi9OuXbsy7fPJJ5+oQoUKVqFDulGjRunixYvasmXLXdWRnJysxYsXS5KcnZ0lScuXL5fJZNLLL7+c4TZFihS5q2NmJj4+XpLk5eWVZb+kpCQlJCRYLQAAAABgL4IH3DMbN26UyWSyWqZMmWJZX6JECU2ZMkUrV67UwIED8/Sv8J07d1b37t3VoEED+fj4qEOHDpo3b57VQ/SpU6dUqVKlDLdPbz916lSujl+3bl2ZTCa5urpq1KhRKlOmjLp06SJJOn36tB5//HFLEJGV+Ph4m2vYrFmzHNdjNps1cuRI1atXT4GBgVn2DQsLk6enp2Xx8/PL8fEAAAAAPLx41QL3TKNGjbRgwQKrtjv/2p6amqqlS5fKzc1N+/btU0pKiozGvPkVdXJyUmRkpKZOnapt27Zp3759mjZtmmbOnKkDBw7Ix8fHrv0YDIZcHX/VqlUKCAjQqVOnNGLECC1cuNBy7maz2e79uru764cffrBqK1SoUI7rGTp0qH766Sft2bMn276hoaEaOXKk5eeEhATCBwAAAAB2Y8QD7pnChQvL39/farkzeHjnnXd0+vRpHTx4UOfPn9f06dPzvIaSJUuqV69eev/993X8+HHdvHlTCxculCRVqFBBx48fz3C7EydOSJLKly8v6fYkj+mvKtwpfT4GT09Pq3Y/Pz+VL19erVu31ocffqiuXbvqr7/+shz3v//9r5KTk7Otv0CBAjbXsGTJkvad/P83bNgwrV+/Xtu3b1epUqWy7e/i4iIPDw+rBQAAAADsRfCAf4Vjx45p4sSJWrBggSpXrqyFCxdq6tSp+umnnxx2zKJFi8rHx8fyVYdu3brp9OnTGX7qctasWSpWrJiaNm0qSQoICNAff/yhCxcuWPU7ePCgJRzITIMGDRQYGKhp06ZJknr06KFr165p/vz5Gfa/c3LJu2E2mzV06FCtXbtW27ZtU9myZfNkvwAAAACQFV61wD2TlJRk86BuNBpVpEgRBQcHq0OHDurUqZMkqX379urcubNCQkJ04MCBu37lYtGiRYqJiVGHDh1Urlw53bx5U8uWLdOxY8c0d+5cSbeDh9WrVys4OFhvv/22GjdurISEBL3//vtav369Vq9ebfmiRbNmzVSpUiV169ZN06ZNk6+vr3766SeNHj1agwYNkru7e5b1jBo1Sp07d9bYsWP19NNPa+zYsRo1apTOnTunDh06yNfXV7/88osWLlyoevXqWb52YTabba6hdHt+jAIFss4RhwwZok8++USff/653N3dLfvx9PTM1esaAAAAAGAPggfcM5s2bbKZS6FixYrq0aOHzp07p6+//tpq3dy5c1WlShVNnz5dEyZMuKtj165dW3v27NGgQYN0/vx5mUwmValSRevWrVODBg0k3Z6/ITo6WnPmzNHs2bM1ZMgQubi46JlnntH27dtVr149y/6MRqM2b96s8ePHq2fPnvrrr79UunRp9e/fX2PHjs22njZt2qhMmTKaNm2a5s+fr5kzZ+qpp57S+++/r4ULFyotLU3lypVTp06dFBwcbNkuISEhw/ko4uLisv3MZvr8Gg0bNrRqj4yMVEhISLY1AwAAAEBuGMxmszm/iwBw/0hISLj9dYsR0Srg4pbf5QAAAAAPlNgZrfO7BLulPxvEx8dnORccczwAAAAAAACHIXjAA2H69OkymUwZLi1btszv8hzu7NmzmZ6/yWTS2bNn87tEAAAAAA8p5njAA2HQoEHq0qVLhusehokTfX19FRMTk+V6AAAAAMgPBA94IHh5ecnLyyu/y8g3RqMxy094AgAAAEB+4VULAAAAAADgMAQPAAAAAADAYQgeAAAAAACAwxA8AAAAAAAAhyF4AAAAAAAADkPwAAAAAAAAHIbPaQLIlaNvNpeHh0d+lwEAAADgX44RDwAAAAAAwGEIHgAAAAAAgMMQPAAAAAAAAIcheAAAAAAAAA5D8AAAAAAAAByG4AEAAAAAADgMwQMAAAAAAHAYY34XAOD+FDjxaxVwccvvMgAAAICHRuyM1vldQq4w4gEAAAAAADgMwQMAAAAAAHAYggcAAAAAAOAwBA8AAAAAAMBhCB4AAAAAAIDDEDwAAAAAAACHIXgAAAAAAAAOQ/AAAAAAAAAchuABAAAAAAA4DMEDAAAAAABwGIIHAAAAAADgMAQPAAAAAADAYQgeAAAAAACAwxA84J4ICQmRwWCwWVq0aKHz58/Ly8tL7733ntU2+/fvl7Ozs7Zs2ZLhtncuISEh2dawfft2NWrUSF5eXnJzc1P58uUVHByslJQUS5/U1FTNnj1b1apVk6urq4oUKaKWLVvq22+/tdrXpEmTVKNGDZtjXLlyRQaDQTt27JAkxcbGWtXp6empOnXqaMOGDTbbrlmzRg0bNpSnp6dMJpOqVaumyZMn69KlS5KkqKioDM/d1dU123OXpF27dqlt27by9fWVwWDQunXr7NoOAAAAAO4GwQPumRYtWiguLs5qWbFihXx9ffXee+8pNDRUp0+fliTduHFDwcHB6t+/v5o2bWq1TXh4uDw8PKza5syZk+Wxjx07ppYtW6pWrVratWuXjhw5orlz58rZ2VlpaWmSJLPZrG7dumny5Ml65ZVXdOLECe3cuVN+fn5q2LDhXT2ob926VXFxcdq/f79q166tjh076ujRo5b1r7/+urp27apatWrpq6++0tGjRzVr1iwdPnxYH330kaXfP887Li5Ov/32m101JCYmqnr16po3b16uzwMAAAAAcsqY3wXg4eHi4iJvb+8M1wUFBWnt2rUKCQnR7t27FRoaqlu3buntt9+WJKvtPD09ZTAYMt1XRrZs2SIfHx+99dZblrZy5cqpRYsWlp+jo6P16aefav369Wrbtq2l/YMPPtDFixctIUjhwoXtPm66YsWKydvbW97e3po2bZrmzp2r7du3KzAwUAcOHND06dMVHh6u4cOHW7YpU6aMmjZtqitXrljacnred2rZsqVatmyZq20BAAAAILcY8YB/jYULF+r06dPq2bOn5s2bp6ioKJlMpjzZt7e3t+Li4rRr165M+3zyySeqUKGCVeiQbtSoUbp48aK2bNlyV3UkJydr8eLFkiRnZ2dJ0vLly2UymfTyyy9nuE2RIkXu6ph3KykpSQkJCVYLAAAAANiL4AH3zMaNG2UymayWKVOmWNaXKFFCU6ZM0cqVKzVw4EDVr18/z47duXNnde/eXQ0aNJCPj486dOigefPmWT1Enzp1SpUqVcpw+/T2U6dO5er4devWlclkkqurq0aNGqUyZcqoS5cukqTTp0/r8ccftwQRWYmPj7e5hs2aNctVTfYKCwuTp6enZfHz83Po8QAAAAA8WHjVAvdMo0aNtGDBAqs2Ly8vy79TU1O1dOlSubm5ad++fUpJSZHRmDe/ok5OToqMjNTUqVO1bds27du3T9OmTdPMmTN14MAB+fj42LUfg8GQq+OvWrVKAQEBOnXqlEaMGKGFCxdazt1sNtu9X3d3d/3www9WbYUKFcpVTfYKDQ3VyJEjLT8nJCQQPgAAAACwGyMecM8ULlxY/v7+VsudwcM777yj06dP6+DBgzp//rymT5+e5zWULFlSvXr10vvvv6/jx4/r5s2bWrhwoSSpQoUKOn78eIbbnThxQpJUvnx5SbcneYyPj7fplz4fg6enp1W7n5+fypcvr9atW+vDDz9U165d9ddff1mO+9///lfJycnZ1l+gQAGba1iyZEn7Tj6XXFxc5OHhYbUAAAAAgL0IHvCvcOzYMU2cOFELFixQ5cqVtXDhQk2dOlU//fSTw45ZtGhR+fj4KDExUZLUrVs3nT59OsNPXc6aNUvFihVT06ZNJUkBAQH6448/dOHCBat+Bw8etIQDmWnQoIECAwM1bdo0SVKPHj107do1zZ8/P8P+d04uCQAAAAD3G161wD2TlJRk86BuNBpVpEgRBQcHq0OHDurUqZMkqX379urcubNCQkJ04MCBu37lYtGiRYqJiVGHDh1Urlw53bx5U8uWLdOxY8c0d+5cSbeDh9WrVys4OFhvv/22GjdurISEBL3//vtav369Vq9ebfmiRbNmzVSpUiV169ZN06ZNk6+vr3766SeNHj1agwYNkru7e5b1jBo1Sp07d9bYsWP19NNPa+zYsRo1apTOnTunDh06yNfXV7/88osWLlyoevXqWb52YTabba6hdHt+jAIFss4Rr127pl9++cXy85kzZxQTEyMvLy899thjObqeAAAAAGAvggfcM5s2bbKZS6FixYrq0aOHzp07p6+//tpq3dy5c1WlShVNnz5dEyZMuKtj165dW3v27NGgQYN0/vx5mUwmValSRevWrVODBg0k3Z6/ITo6WnPmzNHs2bM1ZMgQubi46JlnntH27dtVr149y/6MRqM2b96s8ePHq2fPnvrrr79UunRp9e/fX2PHjs22njZt2qhMmTKaNm2a5s+fr5kzZ+qpp57S+++/r4ULFyotLU3lypVTp06dFBwcbNkuISEhw/ko4uLisv3M5qFDh9SoUSPLz+nzNgQHBysqKirbmgEAAAAgNwxms9mc30UAuH8kJCTc/rrFiGgVcHHL73IAAACAh0bsjNb5XYKV9GeD+Pj4LOeCY44HAAAAAADgMAQPeCBMnz5dJpMpw6Vly5b5XZ7DnT17NtPzN5lMOnv2bH6XCAAAAOAhxRwPeCAMGjRIXbp0yXBdoUKF7nE1956vr69iYmKyXA8AAAAA+YHgAQ8ELy8veXl55XcZ+cZoNGb5CU8AAAAAyC+8agEAAAAAAByG4AEAAAAAADgMwQMAAAAAAHAYggcAAAAAAOAwBA8AAAAAAMBhCB4AAAAAAIDD8DlNALly9M3m8vDwyO8yAAAAAPzLMeIBAAAAAAA4DMEDAAAAAABwGIIHAAAAAADgMAQPAAAAAADAYQgeAAAAAACAwxA8AAAAAAAAhyF4AAAAAAAADkPwAAAAAAAAHIbgAQAAAAAAOAzBAwAAAAAAcBiCBwAAAAAA4DAEDwAAAAAAwGEIHgAAAAAAgMMQPAAAAAAAAIcheAAAAAAAAA5D8AAAAAAAAByG4AEAAAAAADgMwQMAAAAAAHAYggcAAAAAAOAwxvwuAMD9xWw2S5ISEhLyuRIAAAAA+Sn9mSD9GSEzBA8AcuTixYuSJD8/v3yuBAAAAMC/wdWrV+Xp6ZnpeoIHADni5eUlSTp79myW/+WCB0dCQoL8/Pz0+++/y8PDI7/LwT3APX/4cM8fPtzzhw/3/OFzL+652WzW1atX5evrm2U/ggcAOVKgwO2pYTw9Pfl/Wg8ZDw8P7vlDhnv+8OGeP3y45w8f7vnDx9H33J4/RjK5JAAAAAAAcBiCBwAAAAAA4DAEDwByxMXFRRMnTpSLi0t+l4J7hHv+8OGeP3y45w8f7vnDh3v+8Pk33XODObvvXgAAAAAAAOQSIx4AAAAAAIDDEDwAAAAAAACHIXgAAAAAAAAOQ/AAAAAAAAAchuABgN3mz5+vsmXLytXVVU899ZR2796d3yUhj4SFhalWrVpyd3dXiRIl1L59e508edKqj9ls1qRJk+Tr66tChQqpYcOGOnbsWD5VjLwWFhYmg8GgESNGWNq45w+ec+fOKSgoSMWKFZObm5tq1Kih77//3rKee/5gSUlJ0RtvvKGyZcuqUKFCevzxxzV58mSlpaVZ+nDP72+7du1S27Zt5evrK4PBoHXr1lmtt+f+JiUladiwYSpevLgKFy6sF154QX/88cc9PAvkVFb3PTk5WePGjVPVqlVVuHBh+fr6qnfv3jp//rzVPu71fSd4AGCXVatWacSIEXr99df1448/6rnnnlPLli119uzZ/C4NeWDnzp0aMmSI9u3bpy1btiglJUXNmjVTYmKipc9bb72ld999V/PmzdPBgwfl7e2tpk2b6urVq/lYOfLCwYMH9cEHH6hatWpW7dzzB8vly5f17LPPytnZWV999ZWOHz+uWbNmqUiRIpY+3PMHy8yZM7Vw4ULNmzdPJ06c0FtvvaW3335bc+fOtfThnt/fEhMTVb16dc2bNy/D9fbc3xEjRuizzz7TypUrtWfPHl27dk1t2rRRamrqvToN5FBW9/369ev64Ycf9J///Ec//PCD1q5dq1OnTumFF16w6nfP77sZAOxQu3Zt86BBg6zaAgICzK+99lo+VQRH+uuvv8ySzDt37jSbzWZzWlqa2dvb2zxjxgxLn5s3b5o9PT3NCxcuzK8ykQeuXr1qLl++vHnLli3mBg0amIcPH242m7nnD6Jx48aZ69Wrl+l67vmDp3Xr1ua+fftatb344ovmoKAgs9nMPX/QSDJ/9tlnlp/tub9XrlwxOzs7m1euXGnpc+7cOXOBAgXMmzZtume1I/f+ed8zcuDAAbMk82+//WY2m/PnvjPiAUC2bt26pe+//17NmjWzam/WrJm+++67fKoKjhQfHy9J8vLykiSdOXNGFy5csPodcHFxUYMGDfgduM8NGTJErVu3VpMmTazauecPnvXr16tmzZrq3LmzSpQooSeeeEKLFy+2rOeeP3jq1aunb775RqdOnZIkHT58WHv27FGrVq0kcc8fdPbc3++//17JyclWfXx9fRUYGMjvwAMkPj5eBoPBMsItP+670SF7BfBA+d///qfU1FQ9+uijVu2PPvqoLly4kE9VwVHMZrNGjhypevXqKTAwUJIs9zmj34HffvvtnteIvLFy5Ur98MMPOnjwoM067vmD59dff9WCBQs0cuRIjR8/XgcOHNArr7wiFxcX9e7dm3v+ABo3bpzi4+MVEBAgJycnpaamatq0aerevbsk/nP+oLPn/l64cEEFCxZU0aJFbfrwv/EeDDdv3tRrr72mHj16yMPDQ1L+3HeCBwB2MxgMVj+bzWabNtz/hg4dqp9++kl79uyxWcfvwIPj999/1/Dhw7V582a5urpm2o97/uBIS0tTzZo1NX36dEnSE088oWPHjmnBggXq3bu3pR/3/MGxatUqffzxx/rkk09UpUoVxcTEaMSIEfL19VVwcLClH/f8wZab+8vvwIMhOTlZ3bp1U1pamubPn59tf0fed161AJCt4sWLy8nJySYB/euvv2xSdNzfhg0bpvXr12v79u0qVaqUpd3b21uS+B14gHz//ff666+/9NRTT8loNMpoNGrnzp167733ZDQaLfeVe/7g8PHxUeXKla3aKlWqZJkkmP+cP3jGjBmj1157Td26dVPVqlXVq1cvvfrqqwoLC5PEPX/Q2XN/vb29devWLV2+fDnTPrg/JScnq0uXLjpz5oy2bNliGe0g5c99J3gAkK2CBQvqqaee0pYtW6zat2zZorp16+ZTVchLZrNZQ4cO1dq1a7Vt2zaVLVvWan3ZsmXl7e1t9Ttw69Yt7dy5k9+B+1Tjxo115MgRxcTEWJaaNWuqZ8+eiomJ0eOPP/7/2rvzmCjOP47jny0Ki7iO7hKoRxa1oqhgi6AWiUL/aFTqHcWDELfaJjb1qleiSL2QaGyNsa2Ji1FoE0NMvM8YoyYYJShIREOq9QimoZFGGkUbj3X6R8Pktz/RYuuK4PuVbDLMPPPwnfmy2dkvz8xDzluY5OTkZ6bJvXLliqKioiTxPm+JHjx4oHfe8b/cDwoKsqbTJOctW2Pym5CQoNatW/u1qa6u1qVLl/gbaMbqiw5Xr17V8ePH5XK5/LY3Rd651QJAo8yfP1+ZmZlKTExUUlKSvF6vqqqqNHPmzKYODa/Al19+qR07dmjfvn1yOBzWf0cMw1BoaKhsNpvmzZun3NxcRUdHKzo6Wrm5uWrTpo2mTp3axNHj33A4HNYzPOqFhYXJ5XJZ68l5y/LVV19p8ODBys3NVXp6ukpKSuT1euX1eiWJ93kLNGrUKK1Zs0Zut1t9+/bVhQsXtGHDBk2fPl0SOW8J6urq9Msvv1g/37hxQ+Xl5XI6nXK73f+YX8MwNGPGDC1YsEAul0tOp1MLFy5UXFzcMw8dxpvjRXnv1KmTJkyYoLKyMh08eFA+n8+6rnM6nQoODm6avAdkrgwALdIPP/xgRkVFmcHBwWb//v2tqRbR/Elq8LV9+3arzdOnT83ly5eb7777rhkSEmIOHTrUrKioaLqg8cr973SapknOW6IDBw6YsbGxZkhIiBkTE2N6vV6/7eS8Zbl79645d+5c0+12m3a73ezevbuZlZVlPnz40GpDzpu3kydPNvj5PW3aNNM0G5ffP//805w1a5bpdDrN0NBQc+TIkWZVVVUTHA0a60V5v3HjxnOv606ePGn18brzbjNN0wxMSQMAAAAAALzteMYDAAAAAAAIGAoPAAAAAAAgYCg8AAAAAACAgKHwAAAAAAAAAobCAwAAAAAACBgKDwAAAAAAIGAoPAAAAAAAgICh8AAAAAAAAAKGwgMAAAAAAAgYCg8AAABvMI/Ho7FjxzZ1GA26efOmbDabysvLmzoUAMAbjMIDAAAAXtqjR4+aOgQAQDNB4QEAAKCZSE1N1ezZszVv3jx16NBBkZGR8nq9un//vj799FM5HA699957OnLkiLXPqVOnZLPZdOjQIb3//vuy2+0aNGiQKioq/PretWuX+vbtq5CQEHXt2lXffvut3/auXbsqJydHHo9HhmHo888/V7du3SRJ8fHxstlsSk1NlSSdO3dOH3/8scLDw2UYhlJSUlRWVubXn81m09atWzVu3Di1adNG0dHR2r9/v1+by5cv65NPPlG7du3kcDg0ZMgQXbt2zdq+fft29e7dW3a7XTExMdq8efN/PscAgFePwgMAAEAzUlBQoPDwcJWUlGj27Nn64osvNHHiRA0ePFhlZWUaNmyYMjMz9eDBA7/9Fi1apG+++Ubnzp1TRESERo8ercePH0uSSktLlZ6ersmTJ6uiokIrVqxQdna28vPz/fpYv369YmNjVVpaquzsbJWUlEiSjh8/rurqau3evVuSdO/ePU2bNk1FRUUqLi5WdHS00tLSdO/ePb/+Vq5cqfT0dF28eFFpaWnKyMjQnTt3JEm//vqrhg4dKrvdrhMnTqi0tFTTp0/XkydPJEl5eXnKysrSmjVrVFlZqdzcXGVnZ6ugoOCVn3MAwH9jM03TbOogAAAA0DCPx6M//vhDe/fuVWpqqnw+n4qKiiRJPp9PhmFo/Pjx+vHHHyVJv/32mzp27KizZ8/qww8/1KlTp/TRRx+psLBQkyZNkiTduXNHXbp0UX5+vtLT05WRkaGamhodO3bM+r2LFy/WoUOHdPnyZUl/j3iIj4/Xnj17rDY3b95Ut27ddOHCBX3wwQfPPQafz6cOHTpox44dGjlypKS/RzwsW7ZMq1evliTdv39fDodDhw8f1vDhw7V06VIVFhbq559/VuvWrZ/p0+12a926dZoyZYq1LicnR4cPH9aZM2f+zakGAAQIIx4AAACakX79+lnLQUFBcrlciouLs9ZFRkZKkm7fvu23X1JSkrXsdDrVq1cvVVZWSpIqKyuVnJzs1z45OVlXr16Vz+ez1iUmJjYqxtu3b2vmzJnq2bOnDMOQYRiqq6tTVVXVc48lLCxMDofDiru8vFxDhgxpsOhQU1OjW7duacaMGWrbtq31ysnJ8bsVAwDwZmjV1AEAAACg8f7/i7jNZvNbZ7PZJElPnz79x77q25qmaS3Xa2hQbFhYWKNi9Hg8qqmp0caNGxUVFaWQkBAlJSU980DKho6lPu7Q0NDn9l/fJi8vT4MGDfLbFhQU1KgYAQCvD4UHAACAt0BxcbHcbrckqba2VleuXFFMTIwkqU+fPjp9+rRf+zNnzqhnz54v/CIfHBwsSX6jIiSpqKhImzdvVlpamiTp1q1b+v33318q3n79+qmgoECPHz9+pkARGRmpzp076/r168rIyHipfgEArx+FBwAAgLfAqlWr5HK5FBkZqaysLIWHh2vs2LGSpAULFmjAgAFavXq1Jk2apLNnz+r777//x1kiIiIiFBoaqqNHj6pLly6y2+0yDEM9evTQTz/9pMTERN29e1eLFi164QiGhsyaNUvfffedJk+erCVLlsgwDBUXF2vgwIHq1auXVqxYoTlz5qhdu3YaMWKEHj58qPPnz6u2tlbz58//t6cJABAAPOMBAADgLbB27VrNnTtXCQkJqq6u1v79+60RC/3799fOnTtVWFio2NhYff3111q1apU8Hs8L+2zVqpU2bdqkLVu2qFOnThozZowkadu2baqtrVV8fLwyMzM1Z84cRUREvFS8LpdLJ06cUF1dnVJSUpSQkKC8vDxr9MNnn32mrVu3Kj8/X3FxcUpJSVF+fr41xScA4M3BrBYAAAAtWP2sFrW1tWrfvn1ThwMAeAsx4gEAAAAAAAQMhQcAAAAAABAw3GoBAAAAAAAChhEPAAAAAAAgYCg8AAAAAACAgKHwAAAAAAAAAobCAwAAAAAACBgKDwAAAAAAIGAoPAAAAAAAgICh8AAAAAAAAAKGwgMAAAAAAAiYvwBsdAOp1jf3vgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs hyperparamètres: {'bagging_fraction': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'num_leaves': 40}\n",
      "Accuracy: 0.9187831290039349\n",
      "Score métier: 48150\n",
      "AUC: 0.7783389507698268\n",
      "Full model run - done in 10934s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Full model run\"):\n",
    "    importance_df,accuracy,score_metier,auc=kfold_lightgbm(df, 5, stratified = False, debug= False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b280338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modéle load ok\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('modele_lightgbm.pkl', 'rb') as f:\n",
    "    model_lgbm = pickle.load(f)\n",
    "    print(\"modéle load ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37d3981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_test.pkl', 'wb') as f:\n",
    "    pickle.dump(test_df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "237fc146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Fonction objectif pour l'optimisation multi-objectifs\n",
    "def objective1(trial):\n",
    "    # Définition des hyperparamètres à optimiser et de leurs espaces de recherche\n",
    "    params = {\n",
    "        'C': trial.suggest_loguniform('C', 0.1, 10.0),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l2', 'none'])\n",
    "         \n",
    "    }\n",
    "\n",
    "    # Entraînement du modèle avec les hyperparamètres proposés par Optuna\n",
    "    model = LogisticRegression(**params)\n",
    "\n",
    "    # Définition des métriques à évaluer lors de la validation croisée\n",
    "    scoring = {\n",
    "        'auc': make_scorer(roc_auc_score),\n",
    "        'score_metier': make_scorer(custom_cost, greater_is_better=False)\n",
    "    }\n",
    "\n",
    "    # Calcul des scores de validation croisée avec les métriques AUC et score métier\n",
    "    cv_results = cross_validate(model, X_train, y_train, cv=5, scoring=scoring)\n",
    "\n",
    "    # Calcul des métriques moyennes sur les plis de validation croisée\n",
    "    auc = cv_results['test_auc'].mean()\n",
    "    score_metier = -cv_results['test_score_metier'].mean()\n",
    "\n",
    "    # Optimisation multi-objectifs : maximisation de l'AUC et minimisation du score métier\n",
    "    return auc, score_metier\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de validation\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "def logistic_regression_model(X_train_res, y_train_res, X_test, y_test) :\n",
    "    # Création de l'étude Optuna\n",
    "    study = optuna.create_study(directions=['maximize', 'minimize'])\n",
    "\n",
    "    # Optimisation des hyperparamètres avec Optuna\n",
    "    study.optimize(objective1, n_trials=100)\n",
    "\n",
    "    # Obtention des meilleurs essais pour chaque objectif\n",
    "    best_trials = study.best_trials\n",
    "\n",
    "    # Obtention des meilleurs hyperparamètres\n",
    "    best_params = best_trials[0].params\n",
    "\n",
    "    # Terminez l'exécution MLflow précédente si elle est active\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n",
    "\n",
    "    # Démarrez une nouvelle exécution MLflow\n",
    "    mlflow.start_run()\n",
    "\n",
    "    # Enregistrez les paramètres\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Entraînement du modèle avec les meilleurs hyperparamètres pour l'AUC\n",
    "    best_model = LogisticRegression(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Enregistrez le modèle\n",
    "    mlflow.sklearn.log_model(best_model, \"random_forest_model\")\n",
    "\n",
    "    # Prédictions sur les données de test\n",
    "    y_pred = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Conversion des probabilités en classes prédites en utilisant un seuil optimal\n",
    "    threshold = 0.5  # Seuil initial\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    score_metier = custom_cost(y_test, y_pred_binary)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    # Enregistrez les métriques\n",
    "    mlflow.log_metrics({\"accuracy\": accuracy, \"score_metier\": score_metier, \"auc\": auc})\n",
    "\n",
    "    # Terminez l'exécution MLflow\n",
    "    mlflow.end_run()\n",
    "    return accuracy, score_metier, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ebace0cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-12 02:24:26,015] A new study created in memory with name: no-name-38daccce-04d6-40d2-ab7e-70707e010c3e\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 02:24:55,585] Trial 0 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 3.3677889137000907, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:25:22,084] Trial 1 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.1254331607531597, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 02:25:49,605] Trial 2 finished with values: [0.5000696884364757, 115316.4] and parameters: {'C': 0.9260529985441645, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 02:26:17,340] Trial 3 finished with values: [0.5000088892461136, 115333.6] and parameters: {'C': 1.262819669374607, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:26:43,749] Trial 4 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.884314913012811, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:27:10,938] Trial 5 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 2.669974881316828, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:27:39,762] Trial 6 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 3.3963862636483757, 'penalty': 'none'}. \n",
      "[I 2023-07-12 02:28:03,673] Trial 7 finished with values: [0.4999915492079657, 115337.6] and parameters: {'C': 0.5314862198257202, 'penalty': 'l2'}. \n",
      "[I 2023-07-12 02:28:27,474] Trial 8 finished with values: [0.4999915492079657, 115337.6] and parameters: {'C': 0.21733390581189777, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:28:54,077] Trial 9 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 5.913449133209745, 'penalty': 'none'}. \n",
      "[I 2023-07-12 02:29:18,075] Trial 10 finished with values: [0.4999915492079657, 115337.6] and parameters: {'C': 0.10469039859708526, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 02:29:44,349] Trial 11 finished with values: [0.5000132515198616, 115331.8] and parameters: {'C': 1.7950322620696757, 'penalty': 'l2'}. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:30:10,863] Trial 12 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 4.880591393851155, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 02:30:37,091] Trial 13 finished with values: [0.5000088892461136, 115333.6] and parameters: {'C': 9.096294001651112, 'penalty': 'l2'}. \n",
      "[I 2023-07-12 02:31:02,669] Trial 14 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.13197073433526751, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 02:31:28,694] Trial 15 finished with values: [0.49998740504790506, 115333.0] and parameters: {'C': 1.1971839808043125, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:31:55,157] Trial 16 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 2.150604566420106, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:32:21,633] Trial 17 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.43473684278429126, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 02:32:47,787] Trial 18 finished with values: [0.5000132515198616, 115331.8] and parameters: {'C': 0.4698430399139361, 'penalty': 'l2'}. \n",
      "[I 2023-07-12 02:33:13,601] Trial 19 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.6190283049411018, 'penalty': 'l2'}. \n",
      "[I 2023-07-12 02:33:37,565] Trial 20 finished with values: [0.4999915492079657, 115337.6] and parameters: {'C': 5.446758916153585, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:34:04,049] Trial 21 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.9664293314507113, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:34:30,463] Trial 22 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.5461473309986199, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:34:56,861] Trial 23 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.4482265861207932, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:35:23,422] Trial 24 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.3152463145096389, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 02:35:50,130] Trial 25 finished with values: [0.5000218670105138, 115331.4] and parameters: {'C': 0.9431160259635011, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:36:16,417] Trial 26 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 4.76165805247648, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:36:42,844] Trial 27 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.3974820263847424, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:37:09,241] Trial 28 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 3.6712816347063963, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:37:35,603] Trial 29 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 2.1674515469386675, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:38:02,227] Trial 30 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.10610427824558892, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:38:28,561] Trial 31 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 5.70161563345478, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:38:54,988] Trial 32 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.1065279858591508, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:39:21,512] Trial 33 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 6.1504233972465485, 'penalty': 'none'}. \n",
      "[I 2023-07-12 02:39:45,660] Trial 34 finished with values: [0.4999915492079657, 115337.6] and parameters: {'C': 0.17827165711577125, 'penalty': 'l2'}. \n",
      "[I 2023-07-12 02:40:09,866] Trial 35 finished with values: [0.4999915492079657, 115337.6] and parameters: {'C': 0.16204445978041268, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 02:40:35,954] Trial 36 finished with values: [0.5000132515198616, 115331.8] and parameters: {'C': 1.2249401538646192, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:41:02,379] Trial 37 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.6857971478832792, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:41:28,876] Trial 38 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.4719871222839458, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:41:55,435] Trial 39 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.7022373681511869, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:42:22,083] Trial 40 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 9.01786656532497, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 02:42:48,561] Trial 41 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.6091107478502734, 'penalty': 'l2'}. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-12 02:43:12,751] Trial 42 finished with values: [0.4999915492079657, 115337.6] and parameters: {'C': 0.5125873629181018, 'penalty': 'l2'}. \n",
      "[I 2023-07-12 02:43:36,510] Trial 43 finished with values: [0.4999915492079657, 115337.6] and parameters: {'C': 0.2251211568013944, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:44:03,099] Trial 44 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.4260500332298183, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 02:44:29,560] Trial 45 finished with values: [0.5000176683220314, 115328.4] and parameters: {'C': 0.410591085477026, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:44:56,037] Trial 46 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.14191670884321814, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:45:22,823] Trial 47 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 6.5415103475438565, 'penalty': 'none'}. \n",
      "[I 2023-07-12 02:45:49,590] Trial 48 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 3.1766922326182145, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:46:16,000] Trial 49 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.3991393566383463, 'penalty': 'none'}. \n",
      "[I 2023-07-12 02:46:42,394] Trial 50 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.6863581669502433, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:47:09,252] Trial 51 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.12333596718117878, 'penalty': 'none'}. \n",
      "[I 2023-07-12 02:47:35,838] Trial 52 finished with values: [0.5000218670105138, 115331.4] and parameters: {'C': 4.649603422985203, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:48:02,554] Trial 53 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.5320725446394584, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:48:28,992] Trial 54 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.6857971478832792, 'penalty': 'none'}. \n",
      "[I 2023-07-12 02:48:55,489] Trial 55 finished with values: [0.5000218670105138, 115331.4] and parameters: {'C': 4.880591393851155, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:49:21,973] Trial 56 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.10610427824558892, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:49:48,427] Trial 57 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.2822448886059254, 'penalty': 'none'}. \n",
      "[I 2023-07-12 02:50:14,193] Trial 58 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.6447769711850178, 'penalty': 'l2'}. \n",
      "[I 2023-07-12 02:50:38,493] Trial 59 finished with values: [0.4999915492079657, 115337.6] and parameters: {'C': 0.16811481282147892, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:51:05,103] Trial 60 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.4482265861207932, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:51:31,487] Trial 61 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.412808526291929, 'penalty': 'none'}. \n",
      "[I 2023-07-12 02:51:55,342] Trial 62 finished with values: [0.4999915492079657, 115337.6] and parameters: {'C': 5.608392135309996, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:52:21,902] Trial 63 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 8.250948810844024, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 02:52:47,925] Trial 64 finished with values: [0.49999596601013535, 115334.2] and parameters: {'C': 1.963256721248856, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 02:53:13,833] Trial 65 finished with values: [0.49998740504790506, 115333.0] and parameters: {'C': 1.1971839808043125, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:53:40,351] Trial 66 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 8.801088082561725, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 02:54:06,653] Trial 67 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.6647119773971385, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:54:32,914] Trial 68 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 4.480384744848175, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:54:59,344] Trial 69 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 4.165515982343016, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:55:25,702] Trial 70 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 4.76165805247648, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:55:52,193] Trial 71 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 7.814949474510113, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:56:18,708] Trial 72 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 9.01786656532497, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:56:45,017] Trial 73 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.5423418880960703, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:57:11,433] Trial 74 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.30146339581289183, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 02:57:38,212] Trial 75 finished with values: [0.5000218670105138, 115331.4] and parameters: {'C': 0.9442194873315886, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:58:04,506] Trial 76 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.6192313981560391, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:58:30,814] Trial 77 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.884314913012811, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 02:58:57,220] Trial 78 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 5.369877552433513, 'penalty': 'none'}. \n",
      "[I 2023-07-12 02:59:21,109] Trial 79 finished with values: [0.4999915492079657, 115337.6] and parameters: {'C': 0.43473684278429126, 'penalty': 'l2'}. \n",
      "[I 2023-07-12 02:59:47,668] Trial 80 finished with values: [0.5000218670105138, 115331.4] and parameters: {'C': 4.595849481624643, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 03:00:14,113] Trial 81 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.43473684278429126, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 03:00:40,452] Trial 82 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 6.1504233972465485, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-12 03:01:06,967] Trial 83 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.9431160259635011, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 03:01:33,446] Trial 84 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 4.20931107643696, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 03:01:59,620] Trial 85 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.3190733597896815, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 03:02:25,956] Trial 86 finished with values: [0.5000696884364757, 115316.4] and parameters: {'C': 0.9260529985441645, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 03:02:52,687] Trial 87 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.3974820263847424, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 03:03:18,727] Trial 88 finished with values: [0.49999596601013535, 115334.2] and parameters: {'C': 1.8451061141171887, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 03:03:44,789] Trial 89 finished with values: [0.5000696884364757, 115316.4] and parameters: {'C': 0.9260529985441645, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 03:04:10,793] Trial 90 finished with values: [0.5000176683220314, 115328.4] and parameters: {'C': 0.410591085477026, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 03:04:37,147] Trial 91 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.6091107478502734, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 03:05:03,598] Trial 92 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.1255536719558463, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 03:05:30,285] Trial 93 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.34089416294320873, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[I 2023-07-12 03:05:56,577] Trial 94 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.9431160259635011, 'penalty': 'none'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 03:06:22,442] Trial 95 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 1.0931425710678782, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 03:06:48,473] Trial 96 finished with values: [0.5000176683220314, 115328.4] and parameters: {'C': 0.410591085477026, 'penalty': 'l2'}. \n",
      "[I 2023-07-12 03:07:12,405] Trial 97 finished with values: [0.4999915492079657, 115337.6] and parameters: {'C': 9.618267956266703, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 03:07:38,849] Trial 98 finished with values: [0.5000175592651878, 115331.6] and parameters: {'C': 0.6091107478502734, 'penalty': 'l2'}. \n",
      "C:\\Users\\boudj\\anaconda3\\envs\\env2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-12 03:08:05,329] Trial 99 finished with values: [0.4999915492079657, 115337.6] and parameters: {'C': 8.629890936373492, 'penalty': 'l2'}. \n"
     ]
    }
   ],
   "source": [
    "accuracy_lr, score_metier_lr, auc_lr = logistic_regression_model(X_train_res, y_train_res, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "642f81be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43maccuracy_lr\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_lr' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy_lr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
